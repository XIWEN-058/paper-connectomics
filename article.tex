   %\documentclass[wcp,gray]{jmlr} % test grayscale version
\documentclass[wcp]{jmlr}


%% Personnal package
\usepackage{enumerate}
\usepackage{bbold}



 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e

 %\usepackage{rotating}% for sideways figures and tables
% \usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}
\usepackage[utf8]{inputenc}

 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}

% % Define an unnumbered theorem just for this sample document:
%\theorembodyfont{\upshape}
%\theoremheaderfont{\scshape}
%\theorempostheader{:}
%\theoremsep{\newline}
%\newtheorem*{note}{Note}

 % change the arguments, as appropriate, in the following:
\jmlrvolume{1}
\jmlryear{2014}
\jmlrworkshop{\textcolor{red}{Neural Connectomics Workshop}}

% \title[Connectomics challenge]{Inferring neural networks from fluorescent
%                                calcium imaging using partial correlation and
%                                sample weighting \textcolor{red}{plus court} : Simple and robust inference of connectomes using partial correlation coefficients
% ?}

\title[Inference of connectomes using partial correlation coefficients]{Simple and robust inference of connectomes using partial correlation coefficients}


% %%
% Discovery/Infering/Retrieving/... connectomes
% Robust
% Using partial correlation
% [from calcium imaging]


 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
%  \author{\Name{Author Name1\nametag{\thanks{with a note}}} \Email{abc@sample.com}\and
%   \Name{Author Name2} \Email{xyz@sample.com}\\
%   \addr Address}

 % Three or more authors with the same address:
 \author{Todo authors}

%% Proposition (sujet à changements : on en discutera):
% Antonio
% {Aaron, Arnaud, Vincent} : aucune préférence pour ma part
% Gilles
% Damien?
% Pierre?

 % Authors with different addresses:
 % \author{\Name{Author Name1} \Email{abc@sample.com}\\
 % \addr Address 1
 % \AND
 % \Name{Author Name2} \Email{xyz@sample.com}\\
 % \addr Address 2
 %}

\editor{Editor's name}
 % \editors{List of editors' names}

\begin{document}

\maketitle


\begin{abstract} Understanding how the brain works is a key to understand and
to treat brain pathologies and disorders such as Parkinson's disease,
epilepsy. Retrieving the connectomes, neurons connectivity map, will shed new
lights on the anatomical and functional connectivity of the brain. In the
context of the Connectomics challenge, we propose a simple  algorithm made of
four stages, using preprocessing and partial correlation to achieve a network
inference of high quality. We put in perspective our method by comparing
with other inference method such as GTE \cite{stetter2012model}, another
method proposed to infer a neural network from calcium fluorescent imaging,
and Genie3 [], a tree-based approach very competitive in several gene
inference challenge (i.e. DREAM4 and DREAM5).

% Discussion on the metrics?
%\textcolor{red}{Discussion on the order of the partial correlation}

\end{abstract}

\begin{keywords}
Network inference - Partial correlation - Precision matrix - GENIE3
\end{keywords}


\section{Introduction}\label{sec:intro}

% BG : brain, complex organ
The human brain is a complex biological organ, which is formed by 100
billions of neurons with 7000 synaptic connections on average. Inferring the
neuron connectivity network through dissection would be an impossible
and a daunting task. Cutting edge optical devices [todo cite] allow to track
simultaneously the neural activity of thousands of neurons through a fluorescent
calcium indicator.

From those observational data, the neural connectivity inference consists in
highlighting direct relationships between neurons. However, those imaging
systems have a sampling rate smaller than the neuron firing speed and are
limited to two dimensional image, while the neuronal network is a
three dimensional structure, which may lead to superpose the observed activity
of different neurons. Furthermore, the decay of the fluorescent calcium signal
is slower than its rise.

% Introduce notation + goal in mathematical score
% TODO check convention for y_{ij}
One way to emphasize the connections between neurons is to represent them by a
(un)directed graph with $p$ nodes: each node represents a neuron, and each
edge $y_{ij}$ represents a direct link from neuron  $i$ to neuron $j$. If the
graph is undirected, then $y_{ij}$ is equal to $y_{ji}$  for all $i,j$.
The unsupervised task may be formulated as:
\textit{Given $X \in \mathcal{R}^{n \times p}$  a set of $p$ time series of
fluorescent calcium concentration of size $n$ from $p$ neurons, the goal is to
infer the connectivity graph $y \in \left\{0, 1\right\}^{p \times p}$.}
The main challenge in the inference of the network $y$
is to distinguish direct from indirect interactions \cite{de2004discovery}.
For instance with the simple interaction graph $A \rightarrow B \rightarrow C$,
we want to distinguish direct interactions $A \rightarrow B$ and $B \rightarrow C$
from the indirect interaction $A \rightarrow C$.

\textcolor{red}{AJ: Should we keep the following paragraph? While interesting
                this might not be relevant for a 6 page abstract.}
Since only observational data are available, the network recovery is set as an
unsupervised learning problem. Whenever a partial knowledge of the target
network or known networks highly similar to the unknown one is available,
supervised learning [todo cite] and transfer learning  [todo cite] based
approach could be considered to infer networks.

% Solution
In this paper, we describe a simple and a theoretically grounded approach
based on partial correlation to infer the neural connectivity network.
This is also the winning method of the Connectomics challenge
\footnote{\url{http://connectomics.chalearn.org/}}. For reproducibility, the
source code of the method is available at
\url{https://github.com/asutera/kaggle-connectomics}  BSD 3-clauses
license.

\section{Signal filtering} \label{sec:filter}

% Explain why we need to filter

Prior inferring the network, the fluorescent calcium time series
are smoothed using several linear and non linear filters.
Considering the short delay for a communication between neurons, slow events
can be neglected to retrieve direct relationships. The average time
delay is indeed about $1ms$ or $2ms$ while the fluorescent signal time
resolution is $20ms$. Let us also highlight that even if two neurons are
spatially far from each other, the information is expected in the same time
step or at most in the next time step. Thus very low frequency event can be neglected.
However, the image fluorescence calcium signal is also very noisy. Thus,
A low-pass filter (Equation \ref{eq:low-pass}) is first applied followed by a
high-pass filter (Equation \ref{eq:high-pass}), the discrete derivative of
the signal,
\begin{align}
X^\prime_{t,i} &= \sum_{l=-3}^2 c_l^k X_{t+l,i} \forall i, \label{eq:low-pass}\\
X^{\prime\prime}_{t,i} &= X^{\prime}_{t,i} - X^{\prime}_{t-1,i} \forall i, \label{eq:high-pass}
\end{align}
where the $c_l^k$ coefficients were taken in one of the four
following tuples
$\left(c_l^1\right)_{l=-3}^2=(0, 0, 1, 1, 1, 1)$,
$\left(c_l^2\right)_{l=-3}^2=(0, 0, 1, 1, 1, 0)$,
$\left(c_l^3\right)_{l=-3}^2=(1, 1, 1, 1, 0, 0)$ or
$\left(c_l^4\right)_{l=-3}^2=(0.4, 0.8, 1, 1, 0, 0)$.

Connectivity information between two neurons may be retrieved from successive
and slightly delayed burst events. At a local level, direct link
between two neurons will have shorter delay between fluorescence
peaks than undirected links. However, the fluorescence signal has a slow
decay compare to its rise. Therefore, we filter those unrelevant information
with a hard thresholding filter
\begin{align}
X^{\prime\prime\prime}_{t,i} &=
{X^{\prime\prime}_{t,i}}^{0.9} I(X^{\prime\prime}_{t,i} \geq f_3) \forall i
\label{eq:hard-treshold-filter}
\end{align}
where $f_3$ is a parameter of the hard-threshold filter and the $0.9$
exponent reduce the difference between peaks values.

Then we apply on all remaining timesteps $T$ a non-linear filter based on the current
activity of the systems in order to favor the association measure when
a medium size set of neurons are firing together, while artificially
reducing the association measure in case of extreme network activity,
i.e. high when a network firing is happening and low when no neuron is
active:
\begin{equation}
X^{\prime\prime\prime\prime}_{t,i} =
\left\{
  \begin{array}{l}
    1  : r_t < 0\\
    {(X^{\prime\prime\prime}_{t,i} + 1)^{(a_t^{-1} + 1)}}^{g(r_t)} : r_t \geq 0
  \end{array}
\right.
\text{ with }
\left\{
  \begin{array}{ll}
    a_t &= \sum_{i=1}^p X^{\prime\prime\prime}_{t,i} +
                       \frac{1}{2} X^{\prime\prime\prime}_{t-1,i} \forall t \in T\\
    r_t &= \frac{a_t}{\max_{l \in T}{a_l}} \\
  \end{array}
\right.
\end{equation}
where $g(x) = b_1 I(f_4 < x < f_5) + b_2  I(f_5 \leq x < f_6) +
b_3 (I(x\leq f_4) + I(f_ 6 \leq x))$ is a piecewise continuous function of
parameters $b_1, b_2, b_3, f_4, f_5, f_6$. The coefficient were tuned
for each set $i\in\left\{1, 2, 3, 4\right\}$ of coefficient
$\left(c_l^i\right)_{l=-3}^2$.

Finally since there is a high correlation between neurons, we made a
low-rank hypothesis and perform noise filtering through a principal component
approach. From the $p$ eigen vectors, we only kept $80\%$ of
the eigen vectors which have the largest eigen value. In the context of the
challenge, this assumption was verified on all provided dataset

\subsection{Network inference through partial correlation}
\label{sec:inference}


\textcolor{red}{Motivation pour l'inverse : retirer les liens indirects.
Pareil: proposition de AS  et certains choses pourraient être incorporées
dans la version de AJ.}

From a probabilistic point of view, the inference of the undirected network
can be formulated as inferring conditional dependences and independences. Even
if \textit{correlation does no imply causation}[todo cite], it gives insight
on the underlying connectivity network.

Let us consider the neurons as random variables $\mathcal{X}_i$ which follows
a probability density function $p_{\mathcal{X}_i}$ and let us denote by
$p_{\mathcal{X}_i,\mathcal{X}_j|\mathcal{Z}}$ the joint probability
distribution between neurons $i$ and $j$ conditioned on a subset of neurons
$\mathcal{Z}$ (eventually empty). Let us define $\mathcal{V}$ the set of all
neurons $\mathcal{X}_1, \dots, \mathcal{X}_p$, and by $\mathcal{V}^{i,j}$ the
same set without neurons $\mathcal{X}_i$ and $\mathcal{X}_j$.

% Without considering indirect/direct --> Correlation matrix

First, if we want to highlight relationships between neurons without
considering the distinction between direct and indirect links, the Pearson
correlation matrix, i.e. each element $(i,j)$ of a such matrix is the
correlation coefficient between neurons $\mathcal{X}_i$ and $\mathcal{X}_j$, can computed.

% However, we only want direct links --> partial correlation

However the connectivity network to infer is only made of direct relationships
which can be defined as a relation between neurons that still exists when
conditioned on all other neurons or, in other words, there is not intermediary
between those two neurons.  Under the hypothesis that neurons follow a jointly
Gaussian distribution $p_{\mathcal{X}_1, \dots, \mathcal{X}_p} \sim
\mathcal{N}(\mu; \Sigma)$, two neurons $\mathcal{X}_i$ and $\mathcal{X}_j$ are
independent $\mathcal{X}_i\perp \mathcal{X}_j$ if the Pearson correlation
coefficient $\rho_{X_iX_j} = \frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii}
\Sigma_{jj}}} = 0,$ and are conditionally independant to all others neurons
$\mathcal{X}_i \perp \mathcal{X}_j |\mathcal{V}^{i,j}$ if the partial
correlation coefficient (of order $p-2$) $\rho_{X_i, X_j | \mathcal{V}^{i,j}}
= \frac{\Sigma^{-1}_{ij}}{\sqrt{\Sigma^{-1}_{ii} \Sigma^{-1}_{jj}}} = 0.$
Therefore, by definition, the partial correlation coefficient between two
neurons only estimates the direct relationship between them.

In previous study \cite{shipley2002cause}, it has been suggested to
consider correlation coefficients of all possible orders,
i.e. conditioned on every subsets of the set of $p-2$ other variables. This
would take into account multiple indirect paths between a given pair of
variables. An obvious limitation to estimate correctly high-order coefficients
is the number of samples. However given the high number of samples $n$
in the Connectomics challenge, we only consider the highest order of partial
correlation coefficient.

\vspace{1em}
\hrule
\vspace{1em}


% introduce the notion of dependence / independence
From a probabilistic point of view, the inference of the undirected network
can be formulated as inferring conditional dependences and
independences.  Even if \textit{correlation
does no imply causation}[todo cite], it gives insight on the underlying connectivity
network. For instance, the graph $A \rightarrow B \rightarrow C$ translates the
following dependances and independances: $A \not\perp B$, $A \not\perp B | C$,
$B \not\perp C$, $B \not\perp C | A$, but $A \perp C | B$ and  $A \not\perp C$.

Let us consider that each neuron signal $i$ is a random continuous variable
$X_i$ which follows a probability distribution $p_{X_i}$ and let us denote by
$p_{X_i,X_j|\mathcal{Z}}$ the  joint probability distribution of neurons $X_i$
and $X_j$ conditioned on a subset of variables $\mathcal{Z}$ (eventually
empty). Let us define $\mathcal{V}$ the set of all neurons $X_1, ..., X_p$ and
$\mathcal{V}^{-i,j}$ the same set without neurons $X_i$ and $X_j$.

Each neuron signal $i$ is a random continuous variable $X_i$, which follows a
probability density function $p_\mathcal{X_i}(x_i)$. Two neurons $X_i$, $X_j$
will be said conditionally independent to a set of neurons
$\mathcal{V}^{-i,j}$, denoted by  $X_i \perp X_j | \mathcal{V}^{-i,j}$,  if
$p_{X_i,X_j|\mathcal{V}^{-i,j}} = p_{X_i|\mathcal{V}^{-i,j}}
p_{X_j|\mathcal{V}^{-i,j}} $.

%for all real numbers $x_1,\ldots,x_p $ such that
%$p_{X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}}
%(x_1,\ldots,x_p \backslash \left\{x_i, x_j\right\}) > 0$. Note that this
%reciprocal is true only under additional hypotheses.

Under the hypothesis that the neurons follow a jointly Gaussian distribution
$p_{X_1,\ldots,X_p}(x_1,\ldots,x_p) \sim \mathcal{N}(\mu; \Sigma)$ of mean
vector $\mu$ and of covariance matrix $\Sigma$, two neurons $X_i$ and $X_j$
are independent $X_i \perp X_j$ if the Pearson correlation coefficient \[
\rho_{X_iX_j} = \frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii} \Sigma_{jj}}} = 0, \]
and are conditionally independent to all others neurons $X_i \perp X_j |
X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}$ if the partial correlation
coefficient \[ \rho_{X_i, X_j | X_1,\ldots,X_p \backslash \left\{X_i,
X_j\right\}} = \frac{\Sigma^{-1}_{ij}}{\sqrt{\Sigma^{-1}_{ii}
\Sigma^{-1}_{jj}}} = 0. \]

% Correlation => Issue indirect effect
% Partial correlation => better estimate of the graph  => Symmetric measure
While the correlation coefficient is unable to distinguish between
direct and indirect interaction, the partial correlation coefficient
aims to infer only direct ones through the conditioning over all the other
variables. In previous study \cite{shipley2002cause}, it has been suggested to
consider correlation coefficients of all possible orders,
i.e. conditioned on every subsets of the set of $p-2$ other variables. This
would take into account multiple indirect paths between a given pair of
variables. An obvious limitation to estimate correctly high-order coefficients
is the number of samples. However given the high number of samples $n$
in the Connectomics challenge, we only consider the highest order of partial
correlation coefficient.


\subsection{Stacking partial correlation matrix}
\label{sec:stacking}

% Speak about why averaging
The quality of the signal filtering and network inference is dependent upon
good hyper-parameter selection, which are themselves dependent on the properties
of the targeted network and measurement conditions, e.g. signal to noise ratio.
Ensemble methods are known to improve robustness with respect to parameter
selection [todo cite] and improve performance [todo cite].
A weighted average $y_{stack}$ of partial correlation matrices was made
by varying uniformly the hard-threshold value (parameter $f_3$) and the design
of the low-pass filter (coefficients $c_l$). The stacked connectivity score
matrix $y_{stacking}$ is given by
\begin{align}
y_{stack} =  \dfrac{1}{\sum_{k\in\{1,..,4\}} w_k |T_k|} \sum_{k=1}^{k=4}  w_k \sum_{q \in T_k} G^{q,k}(X_t^i)
\end{align}
where $f(X, k, f_3)$ gives the predicted adjacency matrix, i.e. applies the
filtering and inference methodologies (as described
in sections \ref{sec:filter} and \ref{sec:inference}), with a  given value
$f_3$ for the hard-threshold among a set of thresholds $T_k$ of size $|T_k|$
and the $k^{th}$ low-pass filter pattern associated to a weight $w_k$ whose
values are $w_1 = 0.01 , w_2 =1, w_3 = 0.7, w_4 = 0.9$.

\subsection{Causal discovery methodology}
% Introduce a custom solution to make the Y matrix asymmetric
% Introduce directivity
Since the partial correlation is a symmetric measure, the causal mechanism behind the
interaction ($A$ directly causes $B$, $B$ directly causes $A$ or both) can not
be directly inferred\footnote{Also note that two other causal mechanisms might be
implied while having a non-zero partial coefficient: (i) a pair of variables
is induced by a common (hidden) variable; (ii) $A$ (respectively $B$) is
conditionally correlated to a hidden variable affecting $B$ (resp. $A$)
\cite{de2004discovery}.} from a symmetric adjacency matrix.

By ignoring the directivity, we systematically make a true positive and a
false positive for direct link (see a more detailed discussion about the
chosen metric in section \ref{sec:metric}). Therefore, performance might be
improved by predicting an asymmetric adjacency matrix. Hoping to orientate
correctly the most obvious directed links, we attempt to retrieve some causal
information, i.e. the directivity of the links, by computing and stacking with
our symmetric adjacency matrix a matrix of pairwise activation heuristics.
This method computes an anti-symmetric activation score $z_{ij}$ based the
variation of fluorescence signal of a neuron $j$ due to a neuron $i$ over all
timesteps: \[ z_{ij} = \sum_{t=1}^n \mathbb{1}(X_{t+1,j} \ge X_{t, i}) \cdot
\mathbb{1}(f_1\le X_t^i \le f_2) -  \mathbb{1}(X_{t+1,i} \ge X_{t, j}) \cdot
\mathbb{1}(f_1 \le X_t^j \le f_2) \]  where $f_1$ and $f_2$ are parameters of
the method and $I$ is the indicator function. % TODO say that we use only the
3 first filters for this

% TODO speak that we should set the diagonal to the minimum to gain some
% performance.

\section{Experiments}

% Datasets
To assess our method, we have used four datasets provided by the organizer
of the Connectomics challenge : normal-1, normal-2, normal-3, normal-4. Those
datasets are time series of size $n=179500$ of image calcium from simulated
neuronal network \cite{stetter2012model} with $p=1000$ neurons. The networks
to inferred have around 15000 arcs or a density of $0.15\%$ and are similar to those
used to rank challenge participants.

Note that our parameter tuning was mainly on \textit{normal-1} dataset and
partly on \textit{normal-4} dataset, therefore it may justify why these
networks are better retrieved despite the fact that some might be simpler to
infer.

% Metrics : ROC AUC and AUPRC
The network inference task can be viewed as a binary classification task
where one have to correctly classified the presence or the absence of edges
of the graph. The connectivity score matrix
$\hat{y}_{i,j} \in \mathcal{R}$ inferred by a modeling technique
can be thresholded by a value $\tau \in \mathcal{R}$ to evaluate its performance.
Given the ground true network $y_{i,j} \in \left\{0, 1\right\}$, we first define the notion of
true positive $tp(\tau) = \sum_{i=1}^p \sum_{j=1}^p I(y_{i,j} = 1, \hat{y}_{i,j} \geq \tau)$,
true negative $tn(\tau) = \sum_{i=1}^p \sum_{j=1}^p I(y_{i,j} = 0, \hat{y}_{i,j} < \tau)$,
false negative $fn(\tau) = \sum_{i=1}^p \sum_{j=1}^p I(y_{i,j} = 1, \hat{y}_{i,j} < \tau)$ and
false positive $fp(\tau) = \sum_{i=1}^p \sum_{j=1}^p I(y_{i,j} = 0, \hat{y}_{i,j} \geq \tau)$.
Those four numbers formed the confusion matrix.

We assess the accuracy of the network inference using the area under
the ROC curve (ROC AUC) obtained by varying the threshold
$\tau$ of the true positive rate $tpr(\tau) = \frac{tp(\tau)}{tp(\tau) + fn(\tau)}$
as a function of the false positive rate
$fpr(\tau) = \frac{tn(\tau)}{tn(\tau) + fp(\tau)}$. In section (TODO ref),
we also compare the same methods with the area under the precision-recall
curve (AUPRC), an information retrieval metrics, obtained by varying the threshold
$\tau$ of the precision $P(\tau) = \frac{tp(\tau)}{tp(\tau) + fp(\tau)}$
as a function of the recall $R(\tau) = \frac{tp(\tau)}{tp(\tau) + fn(\tau)}$.


\subsection{Data filtering and Signal distribution}

\subsection{Inference of neuronal network}


\begin{table}[htb]
\centering
\caption{Improvement of each stage of our approach Note that we choose the
         \textit{best} threshold for stages 1 to 4.}
\begin{tabular}{*{5}{l}}
\toprule
Stage               & normal-1 & normal-2 & normal-3 & normal-4 \\
\midrule
Inverse correlation & & & & \\
Pre-processing      & & & & \\
PCA                 & & & & \\
Weighting           & & & & \\
Averaging           & & & & \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[htb]
\centering
\caption{Comparison with other methods}
\begin{tabular}{*{5}{l}}
\toprule
Methods             & normal-1 & normal-2 & normal-3 & normal-4 \\
\midrule
Correlation         & & & & \\
Partial correlation & & & & \\
Our approach        & & & & \\
GENIE3              & & & & \\
GTE                 & & & & \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Discussion about the metric}
\label{sec:metric}


\section{Conclusion}

% Possible extensions
% partial autocorrelation function (sometimes "partial correlation function")
% conditional independence test
% Speak about the complexity and parallelization of the methods

\begin{scriptsize}

\paragraph{Acknowledgements.} A. Joly and G. Louppe are research fellows of
the FNRS, Belgium. We also thank Damien for pizzas.  A. Sutera is supported by
a F.R.I.A. scholarship.

\end{scriptsize}

\bibliography{references}


\end{document}
