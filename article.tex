   %\documentclass[wcp,gray]{jmlr} % test grayscale version
\documentclass[wcp]{jmlr}


%% Personnal package
\usepackage{enumerate}



 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e

 %\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}
\usepackage[utf8]{inputenc}

 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}

% % Define an unnumbered theorem just for this sample document:
%\theorembodyfont{\upshape}
%\theoremheaderfont{\scshape}
%\theorempostheader{:}
%\theoremsep{\newline}
%\newtheorem*{note}{Note}

 % change the arguments, as appropriate, in the following:
\jmlrvolume{1}
\jmlryear{2014}
\jmlrworkshop{Neural Connectomics Workshop}

\title[Connectomics challenge]{Inferring neural networks from fluorescent
                               calcium imaging using partial correlation and
                               sample weighting}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
%  \author{\Name{Author Name1\nametag{\thanks{with a note}}} \Email{abc@sample.com}\and
%   \Name{Author Name2} \Email{xyz@sample.com}\\
%   \addr Address}

 % Three or more authors with the same address:
 \author{Todo authors}


 % Authors with different addresses:
 % \author{\Name{Author Name1} \Email{abc@sample.com}\\
 % \addr Address 1
 % \AND
 % \Name{Author Name2} \Email{xyz@sample.com}\\
 % \addr Address 2
 %}

\editor{Editor's name}
 % \editors{List of editors' names}

\begin{document}

\maketitle


\begin{abstract}
Understanding how the brain works is a key to understand and to treat
brain pathologies and disorders such as Parkinson's disease, epilepsy.
Retrieving the connectomes, neurons connectivity map, will shed new lights on
the anatomical and functional connectivity of the brain.
In the context of the Connectomics challenge, we propose a pre-processing and
a simple approach in order to achieve a high performance inference of the
connectivity network.
We put in perspective our method by comparing with other inference method such
as GTE [], another method proposed to infer a neural network from calcium
fluorescent imaging, and Genie3 [], a tree-based approach very competitive
in several gene inference challenge (i.e. DREAM4 and DREAM5).

% Discussion on the metrics?

\end{abstract}

\begin{keywords}
Network inference - Partial correlation - Precision matrix - GENIE3
\end{keywords}


\section{Introduction}\label{sec:intro}

% BG : brain, complex organ
The human brain is a complex biological organ, which is formed by 100
billions of neurons with 7000 synaptic connections on average. Inferring the
neuron connectivity network through dissection would be an impossible
and a daunting task. Cutting edge optical devices [todo cite] allow to track
simultaneously the neural activity of thousands of neurons through a fluorescent
calcium indicator.

From those observational data, the neural connectivity inference consists in
highlighting direct relationships between neurons. However, those imaging
systems have a sampling rate smaller than the neuron firing speed and are
limited to two dimensional image, while the neuronal network is a
three dimensional structure, which may lead to superpose the observed activity
of different neurons. Furthermore, the decay of the fluorescent calcium signal
is slower than its rise.

% Introduce notation + goal in mathematical score
% TODO check convention for y_{ij}
One way to emphasize the connections between neurons is to represent them by a
(un)directed graph with $p$ nodes: each node represents a neuron, and each
edge $y_{ij}$ represents a direct link from neuron  $i$ to neuron $j$. If the
graph is undirected, then $y_{ij}$ is equal to $y_{ji}$  for all $i,j$.
The unsupervised task may be formulated as:
\textit{Given $X \in \mathcal{R}^{n \times p}$  a set of $p$ time series of
fluorescent calcium concentration of size $n$ from $p$ neurons, the goal is to
infer the connectivity graph $y \in \left\{0, 1\right\}^{p \times p}$.}
The main challenge in the inference of the network $y$
is to distinguish direct from indirect interactions \cite{de2004discovery}.
For instance with the simple interaction graph $A \rightarrow B \rightarrow C$,
we want to distinguish direct interactions $A \rightarrow B$ and $B \rightarrow C$
from the indirect interaction $A \rightarrow C$.

Since only observational data are available, the network recovery is set as an
unsupervised learning problem. Whenever a partial knowledge of the target
network or known networks highly similar to the unknown one is available,
supervised learning [todo cite] and transfer learning  [todo cite] based
approach could be considered to infer networks.

% Solution
In this paper, we describe a simple and a theoretically grounded approach
based on partial correlation to infer the neural connectivity network.
This is also the winning method of the Connectomics challenge
\footnote{\url{http://connectomics.chalearn.org/}}.

\section{Signal filtering methodology}
\label{sec:filter}
% Explain why we need to filter
% TODO

% Filtering methodology
Prior inferring the network, the fluorescent calcium time series are smoothed
using several linear and non linear filters. A low-pass filter, a high-pass
filter and a non linear hard-thresholding based filter are
applied for each possible time step $t$
% TODO fix first equation is wrong due notation change
\begin{align}
X^\prime_{t,i} &= \sum_{l=-3}^2 c_l^k X_{t+l,i} \forall i, \\
X^{\prime\prime}_{t,i} &= X^{\prime}_{t,i} - X^{\prime}_{t-1,i} \forall i, \\
X^{\prime\prime\prime}_{t,i} &= {X^{\prime\prime}_{t,i}}^{0.9} I(X^{\prime\prime}_{t,i} \geq f_3) \forall i,
\end{align}
where the $c_l^k$ coefficients were taken in one of the four following tuples
$\left\{c_l^1\right\}=(0, 0, 1, 1, 1, 1)$,
$\left\{c_l^2\right\}=(0, 0, 1, 1, 1, 0)$,
$\left\{c_l^3\right\}=(1, 1, 1, 1, 0, 0)$ or
$\left\{c_l^4\right\}=(0.4, 0.8, 1, 1, 0, 0)$ and $f_3$ is a parameter
of the hard-threshold filter.

Then we apply on all remaining timesteps $T$ a non-linear filter based on the current
activity of the systems in order to favor the association measure when
a medium size set of neurons are firing together, while artificially
reducing the association measure in case of extreme network activity,
i.e. high when a network firing is happening and low when no neuron is
active:
\begin{align}
a_t &= \sum_{i=1}^p X^{\prime\prime\prime}_{t,i} +
                   \frac{1}{2} X^{\prime\prime\prime}_{t-1,i} \forall t \in T\\
r_t &= \frac{a_t}{\max_{l \in T}{a_l}} \\
X^{\prime\prime\prime\prime}_{t,i} &=
\left\{
  \begin{array}{l}
    1  : r_t < 0\\
    {(X^{\prime\prime\prime}_{t,i} + 1)^{(a_t^{-1} + 1)}}^{g(r_t)} : r_t \geq 0
  \end{array}
\right.
\end{align}
where $g(x) = b_1 I(f_4 < x < f_5) + b_2  I(f_5 \leq x < f_6) +
b_3 (I(x\leq f_4) + I(f_ 6 \leq x))$ is a piecewise continuous function of
parameters $b_1, b_2, b_3, f_4, f_5, f_6$. The coefficient were tuned
for each set of coefficient $\left\{c_l^k\right\}$.

Finally since there is a high correlation between neurons, we made a
low-rank hypothesis and perform noise filtering through a principal component
approach [TODO cite]. From the $p$ eigen vectors, we only kept $80\%$ of
the eigen vectors which have the largest eigen value.

\section{Network inference methodology}
\label{sec:inference}

% introduce the notion of dependence / independence
From a probabilistic point of view, the inference of the undirected network
can be formulated as inferring conditional dependences and
independences.  Even if \textit{independence
does no imply causation}[todo cite], it gives insight on the underlying connectivity
network. For instance, the graph $A \rightarrow B \rightarrow C$ translates the
following dependances and independances: $A \not\perp B$, $A \not\perp B | C$,
$B \not\perp C$, $B \not\perp C | A$, but $A \perp C | B$ and  $A \not\perp C$.

Each neuron signal $i$ is a random continuous variable $X_i$,
which follows a probability density function $p_\mathcal{X_i}(x_i)$.
Two neurons $X_i$, $X_j$ will be said conditionally independent
to a set of neurons $X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}$, denoted
by  $X_i \perp X_j | X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}$,  if
\begin{align*}
&p_{X_i, X_j | X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}}
    (x_i, x_j | x_1,\ldots,x_p \backslash \left\{x_i, x_j\right\}) \\
=&p_{X_i | X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}}
    (x_i | x_1,\ldots,x_p \backslash \left\{x_i, x_j\right\})
p_{X_j | X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}}
    (x_j | x_1,\ldots,x_p \backslash \left\{x_i, x_j\right\})
\end{align*}
for all real numbers $x_1,\ldots,x_p $ such that
$p_{X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}}
(x_1,\ldots,x_p \backslash \left\{x_i, x_j\right\}) > 0$. Note that this
reciprocal is true only under additional hypotheses.

Under the hypothesis that the neurons follow a jointly Gaussian distribution
$p_{X_1,\ldots,X_p}(x_1,\ldots,x_p) \sim \mathcal{N}(\mu; \Sigma)$
of mean vector $\mu$ and of covariance matrix $\Sigma$, two neurons $X_i$
and $X_j$ are independent $X_i \perp X_j$ if they have zero Pearson correlation
coefficient
\[
\rho_{X_iX_j} = \frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii} \Sigma_{jj}}}
\]
and are conditionally independent to all others neurons
$X_i \perp X_j | X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}$ if they
have zero partial correlation coefficient
\[
\rho_{X_i, X_j | X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}} =
\frac{\Sigma^{-1}_{ij}}{\sqrt{\Sigma^{-1}_{ii} \Sigma^{-1}_{jj}}}.
\]

% Correlation => Issue indirect effect
% Partial correlation => better estimate of the graph  => Symmetric measure
While the correlation coefficient is unable to distinguish between
direct and indirect interaction, the partial correlation coefficient
aims to infer only direct ones through the conditioning over all the other
variables. In previous study \cite{shipley2002cause}, it has been suggested to
consider correlation coefficients of all possible orders,
i.e. conditioned on every subsets of the set of $p-2$ other variables. This
would take into account multiple indirect paths between a given pair of
variables. An obvious limitation to estimate correctly high-order coefficients
is the number of samples. However given the high number of samples $n$
in the Connectomics challenge, we only consider the highest order of partial
correlation coefficient.




\section{Stacking methodology}
\label{sec:stacking}
% Speak about why averaging
%In order to improve performance, we stacked the partial correlation matrix

Our first guess was that introduce an ensemble method would improve performance
[cite papers which praise ensemble methods]. Among all parameters of our method,
we choose to average over two parameters, i.e. over multiple combinations of the
hard-threshold value $f_3$ and the pattern of the low-pass filter $c_l$. In order to
free ourself from the choice of a probability distribution for the $(f_3,c_l)$ combinations,
we choose a uniform distribution for the threshold distribution (in a given interval of
values) within each filtering process and perform a weighted average for the low-pass
filter pattern. Formally, the stacked predicted adjacency matrix $y_{stacking}$ can be formulated by

\begin{align}
y_{stacking} =  \dfrac{1}{\sum_{k\in\{1,..,4\}} w_k |T_k|} \sum_{k=1}^{k=4}  w_k \sum_{f_3 \in T_k} f(X, k, f_3)
\end{align}
where $f(X, k, f_3)$ gives the predicted adjacency matrix, i.e. applies the filtering and inference methodologies (as described
in sections \ref{sec:filter} and \ref{sec:inference}), with a  given value $f_3$ for the
hard-threshold among a set of thresholds $T_k$ of size $|T_k|$ and the $k^{th}$ low-pass filter pattern associated
to a weight $w_k$ whose values are $w_1 = 0.01 , w_2 =1, w_3 = 0.7, w_4 = 0.9$.

\section{Causal discovery methodology}
% Introduce a custom solution to make the Y matrix asymmetric
% Introduce directivity
Since the partial correlation is a symmetric measure, the causal mechanism behind the
interaction ($A$ directly causes $B$, $B$ directly causes $A$ or both) can not
be inferred\footnote{Also note that two other causal mechanisms might be
implied while having a non-zero partial coefficient: (i) a pair of variables
is induced by a common (hidden) variable; (ii) $A$ (respectively $B$) is
conditionally correlated to a hidden variable affecting $B$ (resp. $A$)
\cite{de2004discovery}.} directly from a symmetric adjacency matrix.

By not taking into account the directivity for directed links, we systematically
make a true positive and a false positive for such links (see a more detailed
discussion about the chosen metric in section \ref{sec:metric}). Therefore,
performance might be improved by predicting an asymmetric adjacency matrix.
Hoping to orientate correctly the most obvious directed links, we attempt to
retrieve some causal information, i.e. the directivity of the links, by computing
and stacking with our symmetric adjacency matrix a matrix of pairwise activation
heuristics. This method computes an anti-symmetric activation score $z_{ij}$
based the variation of fluorescence signal of a neuron $j$ due to a neuron $i$
over all timesteps:
\[
z_{ij} = \sum_{t=1}^n I(X_{t+1,j} - X_{t, i} \in [f_1, f_2]) -  I(X_{t+1,i} - X_{t, j} \in [f_1, f_2])
\]
where $f_1$ and $f_2$ are parameters of the method and $I$ is the
indicator function. % TODO say that we use only the 3 first filters for this


\section{Empirical experiments}
\subsection{Datasets}




\subsection{Metrics}
\subsection{Data filtering}
\subsection{Signal distribution}
\subsection{ROC AUC results}
\paragraph{Improvement of each stage of our approach\\}


\begin{table}[htb]
\centering
\begin{tabular}{|c|c||c|c|c|c|c|c|} \hline
\# & Stage & normal-1 & normal-2 & normal-3 & normal-4 \\ \hline
1 & Inverse correlation & & & & \\ \hline
2 & Pre-processing & & & &\\ \hline
3 & PCA & & & &  \\ \hline
4 & Weighting & & & &  \\ \hline
5 & Averaging & & & &  \\ \hline

\end{tabular}
\end{table}

Note that we choose the \textit{best} threshold for stages 1 to 4.
\paragraph{Comparison with other methods\\}

\begin{table}[htb]
\centering
\begin{tabular}{|c|c||c|c|c|c|} \hline
\# & Stage & normal-1 & normal-2 & normal-3 & normal-4 \\ \hline
1 & Inverse correlation & & & &\\ \hline
2 & Correlation & & & &\\ \hline
3 & GENIE3 & & & & \\ \hline
4 & GTE & & & & \\ \hline

\end{tabular}
\end{table}
\subsection{Discussion about the metric}
\label{sec:metric}


\section{Conclusion}

% Possible extensions
% partial autocorrelation function (sometimes "partial correlation function")
% conditional independence test

\bibliography{references}


\end{document}
