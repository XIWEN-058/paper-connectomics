   %\documentclass[wcp,gray]{jmlr} % test grayscale version
\documentclass[wcp]{jmlr}


%% Personnal package
\usepackage{enumerate}
\usepackage{bbold}



 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e

 %\usepackage{rotating}% for sideways figures and tables
% \usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}
\usepackage[utf8]{inputenc}

 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}

% % Define an unnumbered theorem just for this sample document:
%\theorembodyfont{\upshape}
%\theoremheaderfont{\scshape}
%\theorempostheader{:}
%\theoremsep{\newline}
%\newtheorem*{note}{Note}

 % change the arguments, as appropriate, in the following:
\jmlrvolume{1}
\jmlryear{2014}
\jmlrworkshop{\textcolor{red}{Neural Connectomics Workshop}}

% \title[Connectomics challenge]{Inferring neural networks from fluorescent
%                                calcium imaging using partial correlation and
%                                sample weighting \textcolor{red}{plus court} : Simple and robust inference of connectomes using partial correlation coefficients
% ?}

\title[Inference of connectomes using partial correlation coefficients]{Simple and robust inference of connectomes using partial correlation coefficients}


% %%
% Discovery/Infering/Retrieving/... connectomes
% Robust
% Using partial correlation
% [from calcium imaging]


 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
%  \author{\Name{Author Name1\nametag{\thanks{with a note}}} \Email{abc@sample.com}\and
%   \Name{Author Name2} \Email{xyz@sample.com}\\
%   \addr Address}

 % Three or more authors with the same address:
 \author{Todo authors}

%% Proposition (sujet à changements : on en discutera):
% Antonio
% {Aaron, Arnaud, Vincent} : aucune préférence pour ma part
% Gilles
% Damien?
% Pierre?

 % Authors with different addresses:
 % \author{\Name{Author Name1} \Email{abc@sample.com}\\
 % \addr Address 1
 % \AND
 % \Name{Author Name2} \Email{xyz@sample.com}\\
 % \addr Address 2
 %}

\editor{Editor's name}
 % \editors{List of editors' names}

\begin{document}

\maketitle


\begin{abstract} Understanding how the brain works is a key to understand and
to treat brain pathologies and disorders such as Parkinson's disease,
epilepsy. Retrieving the connectomes, neurons connectivity map, will shed new
lights on the anatomical and functional connectivity of the brain. In the
context of the Connectomics challenge, we propose a simple  algorithm made of
four stages, using preprocessing and partial correlation to achieve a network
inference of high quality. We put in perspective our method by comparing
with other inference method such as GTE \cite{stetter2012model}, another
method proposed to infer a neural network from calcium fluorescent imaging,
and Genie3 [], a tree-based approach very competitive in several gene
inference challenge (i.e. DREAM4 and DREAM5).

% Discussion on the metrics?
%\textcolor{red}{Discussion on the order of the partial correlation}

\end{abstract}

\begin{keywords}
Network inference - Partial correlation - Precision matrix - GENIE3
\end{keywords}


\section{Introduction}\label{sec:intro}

% BG : brain, complex organ
The human brain is a complex biological organ, which is formed by 100
billions of neurons with 7000 synaptic connections on average. Inferring the
neuron connectivity network through dissection would be an impossible
and a daunting task. Cutting edge optical devices [todo cite] allow to track
simultaneously the neural activity of thousands of neurons through a fluorescent
calcium indicator.

From those observational data, the neural connectivity inference consists in
highlighting direct relationships between neurons. However, those imaging
systems have a sampling rate smaller than the neuron firing speed and are
limited to two dimensional image, while the neuronal network is a
three dimensional structure, which may lead to superpose the observed activity
of different neurons. Furthermore, the decay of the fluorescent calcium signal
is slower than its rise.

% Introduce notation + goal in mathematical score
% TODO check convention for y_{ij}
One way to emphasize the connections between neurons is to represent them by a
(un)directed graph with $p$ nodes: each node represents a neuron, and each
edge $y_{ij}$ represents a direct link from neuron  $i$ to neuron $j$. If the
graph is undirected, then $y_{ij}$ is equal to $y_{ji}$  for all $i,j$.
The unsupervised task may be formulated as:
\textit{Given $X \in \mathcal{R}^{n \times p}$  a set of $p$ time series of
fluorescent calcium concentration of size $n$ from $p$ neurons, the goal is to
infer the connectivity graph $y \in \left\{0, 1\right\}^{p \times p}$.}
The main challenge in the inference of the network $y$
is to distinguish direct from indirect interactions \cite{de2004discovery}.
For instance with the simple interaction graph $A \rightarrow B \rightarrow C$,
we want to distinguish direct interactions $A \rightarrow B$ and $B \rightarrow C$
from the indirect interaction $A \rightarrow C$.

\textcolor{red}{AJ: Should we keep the following paragraph? While interesting
                this might not be relevant for a 6 page abstract.}
Since only observational data are available, the network recovery is set as an
unsupervised learning problem. Whenever a partial knowledge of the target
network or known networks highly similar to the unknown one is available,
supervised learning [todo cite] and transfer learning  [todo cite] based
approach could be considered to infer networks.

% Solution
In this paper, we describe a simple and a theoretically grounded approach
based on partial correlation to infer the neural connectivity network.
This is also the winning method of the Connectomics challenge
\footnote{\url{http://connectomics.chalearn.org/}}. For reproducibility, the
source code of the method is available at
\url{https://github.com/asutera/kaggle-connectomics}  BSD 3-clauses
license.

\section{Signal filtering} \label{sec:filter}

% Explain why we need to filter

Prior inferring the network, the fluorescent calcium time series
are smoothed using several linear and non linear filters.
Considering the short delay for a communication between neurons, slow events
can be neglected to retrieve direct relationships. The average time
delay is indeed about $1ms$ or $2ms$ while the fluorescent signal time
resolution is $20ms$. Let us also highlight that even if two neurons are
spatially far from each other, the information is expected in the same time
step or at most in the next time step. Thus very low frequency event can be neglected.
However, the image fluorescence calcium signal is also very noisy. Thus,
A low-pass filter (Equation \ref{eq:low-pass}) is first applied followed by a
high-pass filter (Equation \ref{eq:high-pass}), the discrete derivative of
the signal,
\begin{align}
X^\prime_{t,i} &= \sum_{l=-3}^2 c_l^k X_{t+l,i} \forall i, \label{eq:low-pass}\\
X^{\prime\prime}_{t,i} &= X^{\prime}_{t,i} - X^{\prime}_{t-1,i} \forall i, \label{eq:high-pass}
\end{align}
where the $c_l^k$ coefficients were taken in one of the four
following tuples
$\left(c_l^1\right)_{l=-3}^2=(0, 0, 1, 1, 1, 1)$,
$\left(c_l^2\right)_{l=-3}^2=(0, 0, 1, 1, 1, 0)$,
$\left(c_l^3\right)_{l=-3}^2=(1, 1, 1, 1, 0, 0)$ or
$\left(c_l^4\right)_{l=-3}^2=(0.4, 0.8, 1, 1, 0, 0)$.

Connectivity information between two neurons may be retrieved from successive
and slightly delayed burst events. Direct links
between two neurons should have shorter delay between fluorescence
peaks than undirected links. However, the fluorescence signal has a slow
decay compare to its rise. Therefore, we filter those unrelevant information
with a hard thresholding filter
\begin{align}
X^{\prime\prime\prime}_{t,i} &=
{X^{\prime\prime}_{t,i}}^{0.9} I(X^{\prime\prime}_{t,i} \geq \tau) \forall i
\label{eq:hard-treshold-filter}
\end{align}
where $\tau \geq 0$ is a parameter of the hard-threshold filter and the $0.9$
exponent reduce the difference between peaks values.

Then we apply on all remaining timesteps $T$ a non-linear filter based on the current
activity of the systems in order to favor the association measure when
a medium size set of neurons are firing together, while artificially
reducing the association measure in case of extreme network activity,
i.e. high when a network firing is happening and low when no neuron is
active:
\begin{equation}
X^{\prime\prime\prime\prime}_{t,i} =
\left\{
  \begin{array}{l}
    1  : r_t < 0\\
    {(X^{\prime\prime\prime}_{t,i} + 1)^{(a_t^{-1} + 1)}}^{g(r_t)} : r_t \geq 0
  \end{array}
\right.
\text{ with }
\left\{
  \begin{array}{ll}
    a_t &= \sum_{i=1}^p X^{\prime\prime\prime}_{t,i} +
                       \frac{1}{2} X^{\prime\prime\prime}_{t-1,i} \forall t \in T\\
    r_t &= \frac{a_t}{\max_{l \in T}{a_l}} \\
  \end{array}
\right.
\end{equation}
where $g(x) = b_1 I(f_4 < x < f_5) + b_2  I(f_5 \leq x < f_6) +
b_3 (I(x\leq f_4) + I(f_ 6 \leq x))$ is a piecewise continuous function of
parameters $b_1, b_2, b_3, f_4, f_5, f_6$. The coefficient were tuned
for each set $i\in\left\{1, 2, 3, 4\right\}$ of coefficient
$\left(c_l^i\right)_{l=-3}^2$.

Finally since there is a high correlation between neurons, we made a
low-rank hypothesis and perform noise filtering through a principal component
analysis. From the $p$ eigen vectors, we only kept $80\%$ of
the eigen vectors which have the largest eigen value. In the context of the
challenge, this assumption was deduced from ground truth of provided network.

\subsection{Network inference through partial correlation}
\label{sec:inference}

From a probabilistic point of view, the inference of the undirected network
can be formulated as inferring conditional dependences and independences. Even
if \textit{correlation does no imply causation}[todo cite], it gives insight
on the underlying connectivity network.

Let us consider the neurons as random variables $\mathcal{X}_i$ which follows
a probability density function $p_{\mathcal{X}_i}$ and let us denote by
$p_{\mathcal{X}_i,\mathcal{X}_j|\mathcal{Z}}$ the joint probability
distribution between neurons $i$ and $j$ conditioned on a subset of neurons
$\mathcal{Z}$ (possibly empty). Two neurons $X_i$, $X_j$
will be said conditionally independent to a set of neurons
$\mathcal{Z}$ (possibly empty), denoted by  $X_i \perp X_j | \mathcal{Z}$,  if
$p_{\mathcal{X}_i,\mathcal{X}_j|\mathcal{Z}} = p_{\mathcal{X}_i|\mathcal{Z}}
p_{\mathcal{X}_j|\mathcal{Z}}$. The reciprocal is not true without
additional hypotheses.  Let us define $\mathcal{V}$ the set of all
neurons $\mathcal{X}_1, \dots, \mathcal{X}_p$, and by $\mathcal{V}^{-i,j}$ the
same set without neurons $\mathcal{X}_i$ and $\mathcal{X}_j$.

The connectivity network to infer is only made of direct relationships
which defined relation between neurons that still exists when
conditioned on all other neurons. In other words, there is not intermediary
between those two neurons. Under the hypothesis that neurons follow a jointly
Gaussian distribution $p_{\mathcal{X}_1, \dots, \mathcal{X}_p} \sim
\mathcal{N}(\mu; \Sigma)$, two neurons $\mathcal{X}_i$ and $\mathcal{X}_j$ are
independent $\mathcal{X}_i\perp \mathcal{X}_j$ if the Pearson correlation
coefficient $\rho_{X_iX_j} = \frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii}
\Sigma_{jj}}} = 0,$ and are conditionally independant to all others neurons
$\mathcal{X}_i \perp \mathcal{X}_j |\mathcal{V}^{i,j}$ if the partial
correlation coefficient (of order $p-2$) $\rho_{X_i, X_j | \mathcal{V}^{i,j}}
= \frac{\Sigma^{-1}_{ij}}{\sqrt{\Sigma^{-1}_{ii} \Sigma^{-1}_{jj}}} = 0$,
where $\Sigma^{-1}$ is the inverse of the covariance matrix.
Therefore, by definition, the partial correlation coefficient between two
neurons only estimates the direct relationship between them.

In previous study \cite{shipley2002cause}, it has been suggested to
consider correlation coefficients of all possible orders,
i.e. conditioned on every subsets of the set of $p-2$ other variables. This
would take into account multiple indirect paths between a given pair of
variables. An obvious limitation to estimate correctly high-order coefficients
is the number of samples. However given the high number of samples $n$
in the Connectomics challenge, we only consider the highest order of partial
correlation coefficient.

\subsection{Stacking partial correlation matrix}
\label{sec:stacking}

% Speak about why averaging
The quality of the signal filtering and network inference is dependent upon
good hyper-parameter selection, which are themselves dependent on the properties
of the targeted network and measurement conditions, e.g. signal to noise ratio.
Ensemble methods are known to improve robustness with respect to parameter
selection [todo cite] and improve performance [todo cite].
A weighted average $\hat{y}$ of partial correlation matrices was estimated
from the timeseris $X \in \mathcal{R}^{n \times p}$
by varying unifthe hard-threshold value $\tau \in \mathcal{T}$  and the design
of the low-pass filter (coefficients
$(c_l)_{-3}^2 \forall l \in \left\{1,\ldots,4\right\}$)
\begin{align}
\hat{y} = \frac{1}{\sum_{l=1}^4 w_l |\mathcal{T}|} \sum_{l=1}^4  w_l \sum_{\tau \in \mathcal{T}} G^{\tau,m}(X)
\end{align}
where $G^{k, \tau}(X)$ gives the connectivity score matrix for
a given  $\tau$ and a set of coefficient $c_l$. following
the methodology described in previous section, associated to a weight
$w_p$ whose values are $w_1 = 0.01 , w_2 =1, w_3 = 0.7, w_4 = 0.9$.



\subsection{Causal discovery methodology}
% Introduce a custom solution to make the Y matrix asymmetric
% Introduce directivity
Since the partial correlation is a symmetric measure, the causal mechanism behind the
interaction ($A$ directly causes $B$, $B$ directly causes $A$ or both) can not
be directly inferred\footnote{Also note that two other causal mechanisms might be
implied while having a non-zero partial coefficient: (i) a pair of variables
is induced by a common (hidden) variable; (ii) $A$ (respectively $B$) is
conditionally correlated to a hidden variable affecting $B$ (resp. $A$)
\cite{de2004discovery}.} from a symmetric adjacency matrix.

By ignoring the directivity, we systematically make a true positive and a
false positive for direct link (see a more detailed discussion about the
chosen metric in section \ref{sec:metric}). Therefore, performance might be
improved by predicting an asymmetric adjacency matrix. Hoping to orientate
correctly the most obvious directed links, we attempt to retrieve some causal
information, i.e. the directivity of the links, by computing and stacking with
our symmetric adjacency matrix a matrix of pairwise activation heuristics.
This method computes an anti-symmetric activation score $z_{ij}$ based the
variation of fluorescence signal of a neuron $j$ due to a neuron $i$ over all
timesteps:
\[
z_{ij} = \sum_{t=1}^n \mathbb{1}(X_{t+1,j} \ge X_{t, i}) \cdot
\mathbb{1}(f_1\le X_t^i \le f_2) -  \mathbb{1}(X_{t+1,i} \ge X_{t, j}) \cdot
\mathbb{1}(f_1 \le X_t^j \le f_2)
\]
where $f_1$ and $f_2$ are parameters of the method and $I$ is the indicator
function. Prior computing $z_{ij}$ only the filter defined by Equations
\ref{eq:low-pass}, \ref{eq:high-pass} and \ref{eq:hard-treshold-filter} were
used.

\section{Experiments}

% Datasets
To assess our method, we have used four datasets provided by the organizer
of the Connectomics challenge : normal-1, normal-2, normal-3, normal-4. Those
datasets are time series of size $n=179500$ of image calcium from simulated
neuronal network \cite{stetter2012model} with $p=1000$ neurons. The networks
to inferred have around 15000 arcs or a density of $0.15\%$ and are similar to those
used to rank challenge participants. Note that our parameter tuning was mainly
done on the \textit{normal-1} dataset and partly on \textit{normal-4} dataset,
therefore it may justify why these networks are better retrieved despite the
fact that some might be simpler to infer.

% Metrics : ROC AUC and AUPRC
The network inference task can be viewed as a binary classification task
where one has to correctly classified the presence or the absence of edges
of the graph. Given the ground true network $y_{i,j} \in \left\{0, 1\right\}$,
the connectivity score matrix $\hat{y}_{i,j} \in \mathcal{R}$
and threshold value $\tau$, we first define the notion of
true positive $tp(\tau) = \sum_{i=1}^p \sum_{j=1}^p I(y_{i,j} = 1, \hat{y}_{i,j} \geq \tau)$,
true negative $tn(\tau) = \sum_{i=1}^p \sum_{j=1}^p I(y_{i,j} = 0, \hat{y}_{i,j} < \tau)$,
false negative $fn(\tau) = \sum_{i=1}^p \sum_{j=1}^p I(y_{i,j} = 1, \hat{y}_{i,j} < \tau)$ and
false positive $fp(\tau) = n - tp(\tau) - tn(\tau) -fn(\tau)$.

We assess the accuracy of the network inference using the area under
the ROC curve (ROC AUC) obtained by varying the threshold
$\tau$ of the true positive rate $tpr(\tau) = \frac{tp(\tau)}{tp(\tau) + fn(\tau)}$
as a function of the false positive rate
$fpr(\tau) = \frac{tn(\tau)}{tn(\tau) + fp(\tau)}$.  As suggested in
\cite{schrynemackers2013protocols},
we also compare the same methods with the area under the precision-recall
curve (AUPRC) obtained by varying the threshold
$\tau$ of the precision $P(\tau) = \frac{tp(\tau)}{tp(\tau) + fp(\tau)}$
as a function of the recall $R(\tau) = \frac{tp(\tau)}{tp(\tau) + fn(\tau)}$
in section (todo ref).
Since  there is no self connection (neuron connected to itself), we set
the diagonal elements of $\hat{y}_{i,j}$ to the minimum prior evaluation.

\subsection{Data filtering and Signal distribution}

\subsection{Inference of neuronal network}


\begin{table}[H]
\centering
\caption{Inverse correlation (ROC). Improvement of each stage of our approach. Note that we choose the
         \textit{best} threshold (\textcolor{red}{for now 0.11}) for stages 1 to 4.}
\begin{tabular}{*{5}{l}}
\toprule
Stage               & normal-1 & normal-2 & normal-3 & normal-4 \\
\midrule
Nothing             & 0.777 & 0.767 & 0.772 & 0.774 \\
PCA                 & 0.780 & 0.770 & 0.776 & 0.777 \\
Weights             & 0.780 & 0.769 & 0.776 & 0.777 \\
Weights + PCA       & 0.783 & 0.772 & 0.779 & 0.780 \\
H-T                 & 0.893 & 0.891 & 0.891 & 0.886 \\
H-T + PCA           & 0.894 & 0.892 & 0.891 & 0.886 \\
H-T + Weights       & 0.899 & 0.897 & 0.896 & 0.891 \\
H-T + Weights + PCA & 0.900 & 0.898 & 0.896 & 0.892 \\
P-P                 & 0.925 & 0.925 & 0.924 & 0.923 \\
P-P + PCA           & 0.926 & 0.926 & 0.925 & 0.923 \\
P-P + Weights       & 0.931 & 0.930 & 0.928 & 0.927 \\
P-P + Weights + PCA & 0.932 & 0.931 & 0.930 & 0.928 \\
Averaging           & & & & \\
Stacking            & & & & \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Inverse correlation (P-R). Improvement of each stage of our approach. Note that we choose the
         \textit{best} threshold (\textcolor{red}{for now 0.11}) for stages 1 to 4.}
\begin{tabular}{*{5}{l}}
\toprule
Stage               & normal-1 & normal-2 & normal-3 & normal-4 \\
\midrule
Nothing             & 0.070 & 0.064 & 0.068 & 0.072\\
PCA                 & 0.076 & 0.070 & 0.075 & 0.079\\
Weights             & 0.074 & 0.067 & 0.072 & 0.076\\
Weights + PCA       & 0.080 & 0.073 & 0.079 & 0.083\\
H-T                 & 0.264 & 0.260 & 0.269 & 0.241\\
H-T + PCA           & 0.266 & 0.263 & 0.273 & 0.244\\
H-T + Weights       & 0.280 & 0.273 & 0.281 & 0.251\\
H-T + Weights + PCA & 0.284 & 0.278 & 0.285 & 0.255\\
P-P                 & 0.322 & 0.324 & 0.323 & 0.312\\
P-P + PCA           & 0.347 & 0.352 & 0.355 & 0.341\\
P-P + Weights       & 0.333 & 0.334 & 0.327 & 0.313\\
P-P + Weights + PCA & 0.364 & 0.366 & 0.359 & 0.344\\
Averaging           & & & & \\
Stacking            & & & & \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Correlation (ROC). Improvement of each stage of our approach. Note that we choose the
         \textit{best} threshold (\textcolor{red}{for now 0.11}) for stages 1 to 4.}
\begin{tabular}{*{5}{l}}
\toprule
Stage               & normal-1 & normal-2 & normal-3 & normal-4 \\
\midrule
Nothing             & 0.681 & 0.699 & 0.683 & 0.681\\
Weights             & 0.695 & 0.713 & 0.697 & 0.694\\
H-T                 & 0.876 & 0.873 & 0.877 & 0.869\\
H-T + Weights       & 0.886 & 0.884 & 0.891 & 0.877\\
P-P                 & 0.857 & 0.850 & 0.843 & 0.832\\
P-P + Weights       & 0.826 & 0.823 & 0.817 & 0.799\\
Averaging           & & & & \\
Stacking            & & & & \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Correlation (P-R). Improvement of each stage of our approach. Note that we choose the
         \textit{best} threshold (\textcolor{red}{for now 0.11}) for stages 1 to 4.}
\begin{tabular}{*{5}{l}}
\toprule
Stage               & normal-1 & normal-2 & normal-3 & normal-4 \\
\midrule
Nothing             & 0.028 & 0.028 & 0.025 & 0.026\\
Weights             & 0.031 & 0.030 & 0.027 & 0.028\\
H-T                 & 0.143 & 0.128 & 0.150 & 0.129\\
H-T + Weights       & 0.153 & 0.145 & 0.170 & 0.132\\
P-P                 & 0.100 & 0.087 & 0.079 & 0.083\\
P-P + Weights       & 0.079 & 0.071 & 0.067 & 0.064\\
Averaging           & & & & \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Comparison (ROC) with other methods (for now, $t = 0.11$)}
\begin{tabular}{*{5}{l}}
\toprule
Methods             & normal-1 & normal-2 & normal-3 & normal-4 \\
\midrule
Correlation (best)         & 0.886 & 0.884 & 0.891 & 0.877\\
Partial correlation (best) & 0.932 & 0.931 & 0.930 & 0.928 \\
Our approach               & & & & \\
GENIE3                     & & & & \\
GTE                        & & & & \\
Directivity                & 0.535 & 0.526 & 0.533 & 0.535 \\
Our approach + dir         & & & & \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[htb]
\centering
\caption{Comparison (P-R) with other methods (for now, $t = 0.11$)}
\begin{tabular}{*{5}{l}}
\toprule
Methods             & normal-1 & normal-2 & normal-3 & normal-4 \\
\midrule
Correlation (best)         & 0.153 & 0.145 & 0.170 & 0.132 \\
Partial correlation (best) & 0.364 & 0.366 & 0.359 & 0.344 \\
Our approach (best)        & & & & \\
GENIE3                     & & & & \\
GTE                        & & & & \\
Directivity                & 0.012 & 0.012 & 0.012 & 0.012\\
Our approach + dir         & & & & \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discussion about the metric}
\label{sec:metric}


\section{Conclusion}

% Possible extensions
% partial autocorrelation function (sometimes "partial correlation function")
% conditional independence test
% Speak about the complexity and parallelization of the methods




\begin{scriptsize}

\paragraph{Acknowledgements.} A. Joly and G. Louppe are research fellows of
the FNRS, Belgium.  A. Sutera is  is recipient of
a F.R.I.A. fellowship of F.R.S.-FNRS, Belgium.
This work is supported by PASCAL2 and the IUAP DYSCO, initiated by the
Belgian State, Science Policy Office.

\end{scriptsize}

\newpage
\clearpage

% We allow the bibliography to be added on top of the 6 pages.
% Supplementary material may be added in the form or a URL pointed to from
% the paper.
\bibliography{references}


\end{document}
