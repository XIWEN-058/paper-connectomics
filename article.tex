 %\documentclass[wcp,gray]{jmlr} % test grayscale version
\documentclass[wcp]{jmlr}


%% Personnal package
\usepackage{enumerate}



 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e

 %\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}
\usepackage[utf8]{inputenc}

 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}

% % Define an unnumbered theorem just for this sample document:
%\theorembodyfont{\upshape}
%\theoremheaderfont{\scshape}
%\theorempostheader{:}
%\theoremsep{\newline}
%\newtheorem*{note}{Note}

 % change the arguments, as appropriate, in the following:
\jmlrvolume{1}
\jmlryear{2010}
\jmlrworkshop{Neural Connectomics Workshop}

\title[Connectomics challenge]{Inferring neural networks from fluorescent
                               calcium imaging using partial correlation and
                               sample weighting}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
%  \author{\Name{Author Name1\nametag{\thanks{with a note}}} \Email{abc@sample.com}\and
%   \Name{Author Name2} \Email{xyz@sample.com}\\
%   \addr Address}

 % Three or more authors with the same address:
 \author{Todo authors}


 % Authors with different addresses:
 % \author{\Name{Author Name1} \Email{abc@sample.com}\\
 % \addr Address 1
 % \AND
 % \Name{Author Name2} \Email{xyz@sample.com}\\
 % \addr Address 2
 %}

\editor{Editor's name}
 % \editors{List of editors' names}

\begin{document}

\maketitle


\begin{abstract}
Understanding how the brain works is a key to understand and to treat
brain pathologies and disorders such as Parkinson's disease, epilepsy.
Retrieving the connectomes, neurons connectivity map, will shed new lights on
the anatomical and functional connectivity of the brain.
The connectomics challenge

\end{abstract}

\begin{keywords}
List of keywords
\end{keywords}


\section{Introduction}\label{sec:intro}

% BG : brain, complex organ
Understanding how the brain works is a key to understand and to treat
brain pathologies and disorders such as Parkinson's disease, epilepsy.
Retrieving the connectomes, neurons connectivity map, will shed new lights on
the anatomical and functional connectivity of the brain.
The human brain is a complex biological organ, which is formed by 100
billions of neurons with 7000 synaptic connections on average. Inferring the
neuron connectivity network through dissection would be an impossible
and a daunting task.

% Issue : we have data, but need to have to infer the connectome
Cutting edge optical devices [todo cite] allows to track simultaneously
the neural activity of thousands of neurons through the fluorescent
calcium indicator. Those devices have however a sampling rate smaller
than the neuron firing speed and takes only two dimensional image, which may lead
to superpose the activity of different neurons. Furthermore, the
decay of the fluorescent calcium signal is slow compare to its rise.
The neural connectivity inference consists in highlighting, from observational
data, direct relationships between neurons. One way to emphasize the
connections between neurons is to represent them by a (un)directed graph with
$p$ nodes, where each node represents a neuron, and an edge $y_{ij}$ which
represents a direct link from neuron  $i$ to neuron $j$. If the graph is
undirected, then $y_{ij}$ is equal to $y_{ji}$  for all $i,j$.


% Solution
In this paper, we describe a simple and a theoretically grounded approach
based on partial correlation to infer the neural connectivity network.
This is also the winning method of the Connectomics challenge [todo cite].

\section{Method}

\paragraph{Problem definition\\}


% Introduce notation + goal in mathematical score
% TODO check convention for y_{ij}
Formally, the challenge problem may be formulated by the following statement :
\textit{Given $X \in \mathcal{R}^{n \times p}$  a set of $p$ time series of fluorescent
calcium concentration of size $n$ from $p$ neurons, the goal is to infer the
connectivity network $y \in \left\{0, 1\right\}^{p \times p}$ where
non zero values indicate that the neuron $i$ have an outward connection to
the neuron $j$.}


The challenge was stated as an unsupervised learning problem. The particularity
of unsupervised inference is the network recovery solely based on
observational data. Other approaches have been considered to infer biochemical
networks, e.g. gene regulatory network, such as supervised learning [todo cite]
or transfer learning [todo cite], but these learning processes require, as
starting point, a partial knowledge of the target network or known networks
similar to the unknown one.

% Introduce partial correlation
\paragraph{Partial correlation\\}

A common association measure is the Pearson correlation coefficient.
\begin{align}
\rho_{ij} = \dfrac{cov(I,J)}{\sigma_I \sigma_J}
\end{align}
Even if \textit{correlation does not imply causation} [todo cite], it may give
an insight on the underlying connectivity network. Nevertheless, the main
challenge in inferring a network is to distinguish direct from indirect
interactions [de la Fuente et al., 2004]. While the classical correlation
coefficient characterize both direct and indirect interactions, the partial
correlation coefficient quantifies association between variables conditionally
on one or several other variables (the number of variables considered being
the order of the partial correlation coefficient). [Shipley, 2002] suggests to
consider correlation coefficients of all possible orders, i.e. conditioned
on every subsets of the set of $p-2$ other variables, in order to take into
account multiple indirect paths between a given pair of variables.  An obvious
limitation to calculate high-order coefficient is the number of samples in
conditioned data sets. Considering the large number of samples at our disposal
in the connectomics challenge, we only consider the highest order of partial
correlation coefficient. We thus try to estimate partial correlation matrix as
the inferred network where a matrix element, denoted by
$\rho_{ij | \mathcal{V}^{-i,j}}$, is the partial correlation coefficient of
order $p-2$ between neurons $i$ and $j$. Note that this matrix is symmetric
and thus it is impossible to determine the causal mechanism behind the
interaction (I directly causes J, J directly causes I or both
[de la Fuente, 2004])\footnote{Two other causal mechanisms may be implied
by a non-zero partial coefficient. First, the pair of variables may be induced
by a common (hidden) variable and second, I (respectively J) is conditionally
correlated to a hidden variable affecting J (resp. I) [de la Fuente, 2004].} .
A way to find these coefficient [todo cite?] is to take the inverse of the
covariance matrix to get the partial correlation matrix of order $p-2$, also
known as the precision matrix [todo cite] .

% Introduce directivity
\paragraph{Directivity}

% Introduce filtering
\paragraph{Filtering\\}

The filtering process is made of three main parts:

\begin{enumerate}[i.]
\item A low-pass filter: we consider four shapes of local averaging,
\item A high pass filter : we apply the asymmetric discrete derivative,
\item A non-linear filter: we re-weight samples depending on the current
      activity of the systems in order to favour the association measure when
      directly connected neurons are firing together, while artificially
      reducing the association measure in case of extreme network activity,
      i.e. high when a network firing is happening and low when no neuron is
      active.
\end{enumerate}

\section{Empirical experiments}
\subsection{Datasets}




\subsection{Metrics}
\subsection{Data filtering}
\subsection{Signal distribution}
\subsection{ROC AUC results}
\paragraph{Improvement of each stage of our approach\\}


\begin{table}[h!]
\begin{tabular}{|c|c||c|c|c|c|c|c|} \hline
\# & Stage & normal-1 & normal-2 & normal-3 & normal-4 \\ \hline
1 & Inverse correlation & & & & \\ \hline
2 & Pre-processing & & & &\\ \hline
3 & PCA & & & &  \\ \hline
4 & Weighting & & & &  \\ \hline
5 & Averaging & & & &  \\ \hline

\end{tabular}
\end{table}

Note that we choose the \textit{best} threshold for stages 1 to 4.
\paragraph{Comparison with other methods\\}

\begin{table}[h!]
\begin{tabular}{|c|c||c|c|c|c|} \hline
\# & Stage & normal-1 & normal-2 & normal-3 & normal-4 \\ \hline
1 & Inverse correlation & & & &\\ \hline
2 & Correlation & & & &\\ \hline
3 & GENIE3 & & & & \\ \hline
4 & GTE & & & & \\ \hline

\end{tabular}
\end{table}
\subsection{AUPRC}

\section{Conclusion}



\bibliography{references}


\end{document}
