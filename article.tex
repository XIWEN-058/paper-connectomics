   %\documentclass[wcp,gray]{jmlr} % test grayscale version
\documentclass[wcp]{jmlr}


%% Personnal package
\usepackage{enumerate}



 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e

 %\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}
\usepackage[utf8]{inputenc}

 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}

% % Define an unnumbered theorem just for this sample document:
%\theorembodyfont{\upshape}
%\theoremheaderfont{\scshape}
%\theorempostheader{:}
%\theoremsep{\newline}
%\newtheorem*{note}{Note}

 % change the arguments, as appropriate, in the following:
\jmlrvolume{1}
\jmlryear{2014}
\jmlrworkshop{Neural Connectomics Workshop}

\title[Connectomics challenge]{Inferring neural networks from fluorescent
                               calcium imaging using partial correlation and
                               sample weighting}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
%  \author{\Name{Author Name1\nametag{\thanks{with a note}}} \Email{abc@sample.com}\and
%   \Name{Author Name2} \Email{xyz@sample.com}\\
%   \addr Address}

 % Three or more authors with the same address:
 \author{Todo authors}


 % Authors with different addresses:
 % \author{\Name{Author Name1} \Email{abc@sample.com}\\
 % \addr Address 1
 % \AND
 % \Name{Author Name2} \Email{xyz@sample.com}\\
 % \addr Address 2
 %}

\editor{Editor's name}
 % \editors{List of editors' names}

\begin{document}

\maketitle


\begin{abstract}
Understanding how the brain works is a key to understand and to treat
brain pathologies and disorders such as Parkinson's disease, epilepsy.
Retrieving the connectomes, neurons connectivity map, will shed new lights on
the anatomical and functional connectivity of the brain.
In the context of the Connectomics challenge, we propose a pre-processing and a simple approach in order to achieve a high performance inference of the connectivity network.
We put in perspective our method by comparing with other inference method such as GTE [], another method proposed to infer a neural network from calcium fluorescent imaging, and Genie3 [], a tree-based approach very competitive in several gene inference challenge (i.e. DREAM4 and DREAM5).\\

% Discussion on the metrics?

\end{abstract}

\begin{keywords}
Network inference - Precision matrix - GENIE3
\end{keywords}


\section{Introduction}\label{sec:intro}

% BG : brain, complex organ
Understanding how the brain works is a key to understand and to treat
brain pathologies and disorders such as Parkinson's disease and epilepsy.
Retrieving the connectomes, neurons connectivity map, will shed new lights on
the anatomical and functional connectivity of the brain. However,
the human brain is a complex biological organ, which is formed by 100
billions of neurons with 7000 synaptic connections on average. Inferring the
neuron connectivity network through dissection would be an impossible
and a daunting task.

Cutting edge optical devices [todo cite] allows to track simultaneously
the neural activity of thousands of neurons through the fluorescent
calcium indicator.
From those observational data, the neural connectivity inference consists in
highlighting direct relationships between neurons. However, those imaging
systems have a sampling rate smaller than the neuron firing speed and are
limited only two dimensional image, which may lead to superpose the activity
of different neurons. Furthermore, the decay of the fluorescent calcium signal
is slow compare to its rise.

Since only observational data are available, the network recovery is set as an
unsupervised learning problem. Whenever a partial knowledge of the target
network or known networks highly similar to the unknown one is available,
supervised learning [todo cite] and transfer learning  [todo cite] based
approach could be considered to infer networks.


% Solution
In this paper, we describe a simple and a theoretically grounded approach
based on partial correlation to infer the neural connectivity network.
This is also the winning method of the Connectomics challenge [todo cite].

\section{Network inference methodology}

% Introduce notation + goal in mathematical score
% TODO check convention for y_{ij}
One way to emphasize the connections between neurons is to represent them by a
(un)directed graph with $p$ nodes: each node represents a neuron, and each
edge $y_{ij}$ represents a direct link from neuron  $i$ to neuron $j$. If the
graph is undirected, then $y_{ij}$ is equal to $y_{ji}$  for all $i,j$.
The unsupervised task may be formulated as:
\textit{Given $X \in \mathcal{R}^{n \times p}$  a set of $p$ time series of
fluorescent calcium concentration of size $n$ from $p$ neurons, the goal is to
infer the connectivity graph $y \in \left\{0, 1\right\}^{p \times p}$.}
The main challenge in the inference of the network $y$
is to distinguish direct from indirect interactions \cite{de2004discovery}.
For instance with the simple interaction graph $A \rightarrow B \rightarrow C$,
we want to distinguish direct interactions $A \rightarrow B$ and $B \rightarrow C$
from the indirect interaction $A \rightarrow C$.

% introduce the notion of dependence / independence
From a probabilistic point of view, the inference of the undirected network
can be formulated as inferring conditional dependences and
independences.  Even if \textit{independence
does no imply causation}[todo cite], it gives insight on the underlying connectivity
network. For instance, the graph $A \rightarrow B \rightarrow C$ translates the
following dependances and independances: $A \not\perp B$, $A \not\perp B | C$,
$B \not\perp C$, $B \not\perp C | A$, but $A \perp C | B$ and  $A \not\perp C$.

Each neuron signal $i$ is a random continuous variable $X_i$,
which follows a probability density function $p_\mathcal{X_i}(x_i)$.
Two neurons $X_i$, $X_j$ will be said conditionally independent
to a set of neurons $X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}$, denoted
by  $X_i \perp X_j | X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}$,  if
\begin{align*}
&p_{X_i, X_j | X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}}
    (x_i, x_j | x_1,\ldots,x_p \backslash \left\{x_i, x_j\right\}) \\
=&p_{X_i | X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}}
    (x_i | x_1,\ldots,x_p \backslash \left\{x_i, x_j\right\})
p_{X_j | X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}}
    (x_j | x_1,\ldots,x_p \backslash \left\{x_i, x_j\right\})
\end{align*}
for all real numbers $x_1,\ldots,x_p $ such that
$p_{X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}}
(x_1,\ldots,x_p \backslash \left\{x_i, x_j\right\}) > 0$. Note that this
reciprocal is true only under additional hypotheses.

Under the hypothesis that the neurons follow a jointly Gaussian distribution
$p_{X_1,\ldots,X_p}(x_1,\ldots,x_p) \sim \mathcal{N}(\mu; \Sigma)$
of mean vector $\mu$ and of covariance matrix $\Sigma$, two neurons $X_i$
and $X_j$ are independent $X_i \perp X_j$ if they have zero Pearson correlation
coefficient
\[
\rho_{X_iX_j} = \frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii} \Sigma_{jj}}}
\]
and are conditionally independent to all others neurons
$X_i \perp X_j | X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}$ if they
have zero partial correlation coefficient
\[
\rho_{X_i, X_j | X_1,\ldots,X_p \backslash \left\{X_i, X_j\right\}} =
\frac{\Sigma^{-1}_{ij}}{\sqrt{\Sigma^{-1}_{ii} \Sigma^{-1}_{jj}}}.
\]

% Correlation => Issue indirect effect
% Partial correlation => better estimate of the graph  => Symmetric measure
While the correlation coefficient is unable to distinguish between
direct and indirect interaction, the partial correlation coefficient
aims to infer only direct ones through the conditioning over all the other
variables. In previous study \cite{shipley2002cause}, it has been suggested to
consider correlation coefficients of all possible orders,
i.e. conditioned on every subsets of the set of $p-2$ other variables. This
would take into account multiple indirect paths between a given pair of
variables. An obvious limitation to estimate correctly high-order coefficients
is the number of samples. However given the high number of samples $n$
in the Connectomics challenge, we only consider the highest order of partial
correlation coefficient.

Since the partial correlation is a symmetric, the causal mechanism behind the
interaction ($A$ directly causes $B$, $B$ directly causes $A$ or both ) can't
be infered. Moreover, two other causal mechanisms might be implied while having
a non-zero partial coefficient: (i) a pair of variables is induced by a
common (hidden) variable; (ii) $A$ (respectively $B$) is conditionally
correlated to a hidden variable affecting $B$ (resp. $A$)
\cite{de2004discovery}.

% Introduce a custom solution to make the Y matrix asymmetric
% Introduce directivity
In our last submission in hope to improve performance, some causal information
(directivity of the links) were added by stacking the partial correlation score
matrix with an pairwise activation heuristics. This method computes an anti-symmetric
activation score $z_{ij}$ based the variation of fluorescence signal of a neuron
$j$ due to a neuron $i$ over all timesteps:
\[
z_{ij} = \sum_{t=1}^n I(X_{t+1,j} - X_{t, i} \in [f_1, f_2]) -  I(X_{t+1,i} - X_{t, j} \in [f_1, f_2])
\]
where $f_1$ and $f_2$ are parameters of the method and $I$ is the
indicator function.

\section{Signal filtering methodology}

% Explain why we need to filter
% TODO

% Filtering methodology
Prior inferring the network, the fluorescent calicum time series are smoothed
using several linear and non linear filters. A low-pass filter is first applied
for all valid value of $t$
\[
X^\prime_{t,i} = \sum_{l=-3}^2 a_l X_{t+l,i} \forall i
\]
where the $a_l$ coefficient were taken in one of the four following tuples:
$(\left\{a_l\right\}_{l=-3}^2)=(0, 0, 1, 1, 1, 1)$,
$(\left\{a_l\right\}_{l=-3}^2)=(0, 0, 1, 1, 1, 0)$,
$(\left\{a_l\right\}_{l=-3}^2)=(1, 1, 1, 1, 0, 0)$ or
$(\left\{a_l\right\}_{l=-3}^2)=(0.4, 0.8, 1, 1, 0, 0)$.

The next step is to apply a high-pass filter to the signal:
\[
X^{\prime\prime}_{t,i} = X^{\prime}_{t,i} - X^{\prime}_{t-1,i}
\forall i, \forall t \in \left\{2, \ldots, n\right\}
\]


5 kinds
of filtering technique: (i) a low pass filter, (ii) a high pass filter, (iii)
a high thresholding filter

The filtering process is made of three main parts:


% There are 4 filters:
%     1/ smoothing filter (see filtering argument)
%     2/ hard thresholding filter (see threshold argument)
%     3/ non-linear filter: x[i] = x[i] ** 0.9
%     4/ non-linear filter based on the neuron activations


 %  X_new = np.diff(X_new, axis=0)
 %  thresh1 = X_new < threshold * 1
 %  thresh2 = X_new >= threshold * 1
 %  X_new[thresh1] = 0
 %  X_new[thresh2] = pow(X_new[thresh2], 0.9)


\begin{enumerate}[i.]
\item A low-pass filter: we consider four shapes of local averaging,
\item A high pass filter : we apply the asymmetric discrete derivative,
\item A non-linear filter: we re-weight samples depending on the current
      activity of the systems in order to favour the association measure when
      directly connected neurons are firing together, while artificially
      reducing the association measure in case of extreme network activity,
      i.e. high when a network firing is happening and low when no neuron is
      active.
\end{enumerate}

\section{Empirical experiments}
\subsection{Datasets}




\subsection{Metrics}
\subsection{Data filtering}
\subsection{Signal distribution}
\subsection{ROC AUC results}
\paragraph{Improvement of each stage of our approach\\}


\begin{table}[h!]
\begin{tabular}{|c|c||c|c|c|c|c|c|} \hline
\# & Stage & normal-1 & normal-2 & normal-3 & normal-4 \\ \hline
1 & Inverse correlation & & & & \\ \hline
2 & Pre-processing & & & &\\ \hline
3 & PCA & & & &  \\ \hline
4 & Weighting & & & &  \\ \hline
5 & Averaging & & & &  \\ \hline

\end{tabular}
\end{table}

Note that we choose the \textit{best} threshold for stages 1 to 4.
\paragraph{Comparison with other methods\\}

\begin{table}[h!]
\begin{tabular}{|c|c||c|c|c|c|} \hline
\# & Stage & normal-1 & normal-2 & normal-3 & normal-4 \\ \hline
1 & Inverse correlation & & & &\\ \hline
2 & Correlation & & & &\\ \hline
3 & GENIE3 & & & & \\ \hline
4 & GTE & & & & \\ \hline

\end{tabular}
\end{table}
\subsection{AUPRC}

\section{Conclusion}

% Possible extensions
% partial autocorrelation function (sometimes "partial correlation function")
% conditional independence test

\bibliography{references}


\end{document}
