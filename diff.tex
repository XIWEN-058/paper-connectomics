\documentclass[wcp]{jmlr}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL article_old.tex   Wed Jul 16 16:34:16 2014
%DIF ADD article.tex       Wed Jul 16 16:59:19 2014

\usepackage{enumerate}
\usepackage{bbold}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage[utf8]{inputenc}

\jmlryear{2014}
\jmlrworkshop{Neural Connectomics Workshop}

\title{Simple connectome inference from partial correlation statistics in calcium imaging}

  \author{\Name{Antonio Sutera},
   \Name{Arnaud Joly},
   \Name{Vincent François-Lavet}, \Email{a.sutera@ulg.ac.be}\\
   \Name{Zixiao Aaron Qiu},
   \Name{Gilles Louppe},
   \Name{Damien Ernst}\and\Name{Pierre Geurts}
    \\
   \addr Department of EE and CS \& GIGA-R, University of Li\`ege, Belgium}



% \author{\Name{Antonio Sutera,
%     Arnaud Joly,
%         Vincent François-Lavet,\\
%         Aaron Qiu,
%         Gilles Louppe,
%         Damien Ernst,
%         Pierre Geurts}
%   \Email{a.sutera@ulg.ac.be}\\
%   \addr Department of EE and CS \& GIGA-R, University of Li\`ege, Belgium}

% \jmlrauthors{A. Sutera et al}
% \shortauthor{A. Sutera et al.}
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFadd}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}

\maketitle

\DIFaddbegin \vspace{-0.8cm}
\DIFaddend \begin{abstract}
In this work, we propose a simple, but yet efficient method for the problem of
connectome inference in calcium imaging data. The proposed algorithm consists of
two steps. First, processing the raw signals to detect neural peak activities.
Second, inferring the degree of association between neurons from partial
correlation statistics. This paper summarizes the methodology that led us to
win the Connectomics Challenge, proposes a simplified version of our method and
finally discusses our results with respect to other inference methods.
\end{abstract}

\begin{keywords}
Connectomics - Network inference - Partial correlation
\end{keywords}


\section{Introduction}\label{sec:intro}

The human brain is a complex biological
organ made of \DIFaddbegin \DIFadd{about }\DIFaddend 100 billions of neurons, each connected to 7000 other neurons on
average \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{pakkenberg2003aging}
}%DIFAUXCMD
}\DIFaddend . Unfortunately, direct observation of the connectome, the wiring
diagram of the brain, is not yet technically feasible. Without being perfect,
calcium imaging currently allows the real-time and simultaneous observation of
neuron activities from thousands of neurons, producing individual time series
representing their fluorescence intensity. From these data, the connectome
inference problem amounts to retrieving the synaptic connections between
neurons on the basis of the fluorescence time series. This
problem is often difficult because of experimental issues, including
masking effects (i.e., some of the neurons are not observed or confounded with others), the low sampling rate of the optical device with respect to the
neural activity speed or the slow decay of fluorescence.

Formally, the connectome can be represented as a directed graph $G=(V,E)$,
where $V$ is a set of $p$ nodes, representing neurons, and $E \subseteq
\left\{(i, j) \in V \times V\right\}$ is a set of edges, representing direct
synaptic connections between neurons. Causal interactions are expressed by the
direction of edges: $(i, j) \in E$ indicates that the state of neuron $j$ might
be caused by the activity of neuron $i$. In those terms,  the connectome
inference problem is formally stated as follows:  \textit{Given the sampled
observations $\{ x^t_i \in \mathbb{R} | i \in V, t = 1, \dots, T \}$ of $p$
neurons for $T$ time intervals, the goal is to infer the set $E$ of connections
in $G$.}

In this paper, we present a simplified -- yet nearly as good -- version of the
winning method\footnote{Code available at \DIFdelbegin %DIFDELCMD < \url{https://github.com/asutera
%DIFDELCMD < /kaggle-connectomics}%%%
\DIFdelend \DIFaddbegin \url{https://github.com/asutera/kaggle-connectomics}\DIFaddend } of the Connectomics
Challenge\footnote{\url{http://connectomics.chalearn.org}}, as a simple and
theoretically grounded approach based on signal processing techniques and
partial correlation statistics. The paper is structured as follows:
Section~\ref{sec:filter} describes the signal processing methods applied on
fluorescent calcium time series; Section\ref{sec:inference} then presents the
proposed approach and its theoretical properties; Section~\ref{sec:results}
provides an empirical analysis and comparison with other network inference
methods, while Section~\ref{sec:conclusion} finally discusses our work and
provides further research directions. Additionally,
Appendix~\ref{app:optimized} further describes, in all details, our actual
winning method, giving slightly better results than the method presented in
the paper, at the price of parameter tuning. Appendix~\ref{app:supp} provides supplementary results on other datasets.


\section{Signal processing} \label{sec:filter}

Under the simplifying assumption that neurons are on-off units, characterized
by short periods of intense activities, or peaks, and longer periods of
inactivity, the first part of our algorithm consists in cleaning the raw
fluorescence data.
More specifically, time series are processed using standard
signal processing filters in order to \DIFdelbegin \DIFdel{remove noise due }\DIFdelend \DIFaddbegin \DIFadd{(i) remove noise mainly due to fluctuations independent of calcium, calcium fluctuations independent of spiking activity, calcium fluctuations in nearby tissues that have been mistakenly captured, or simply by the imaging process (ii) }\DIFaddend to \DIFdelbegin \DIFdel{light scattering
effects, to }\DIFdelend account for fluorescence low decay and \DIFaddbegin \DIFadd{(iii) }\DIFaddend to reduce the importance of
global high activity in the network. The overall process is illustrated in
Figure~\ref{fig:filtered-signal}.

\begin{figure}
\centering
\subfigure[Raw signal]{\includegraphics[width=0.3\textwidth]{original_curve.pdf} \label{fig:original_curve}}
\subfigure[Low-pass filter $f_1$]{\includegraphics[width=0.3\textwidth]{filter_curve.pdf} \label{fig:lp_curve}}
\subfigure[High-pass filter $g$]{\includegraphics[width=0.3\textwidth]{diff_curve.pdf} \label{fig:hp_curve}}\\
\subfigure[Hard-threshold filter $h$]{\includegraphics[width=0.3\textwidth]{threshold_curve.pdf} \label{fig:threshold_curve}}
\subfigure[Global regularization $w$]{\includegraphics[width=0.3\textwidth]{weights_curve.pdf} \label{fig:weight_curve}}
\caption{Signal processing pipeline for extracting peaks from the raw fluorescence data.}
\label{fig:filtered-signal}
\end{figure}

As Figure~\ref{fig:original_curve} shows, the raw
fluorescence signal is very noisy due to light scattering artifacts that
usually affect the quality of the recording~\citep{lichtman2011big}.
Accordingly, the first step of our pipeline is to smoothen the signal, using
one of the following low-pass filters for filtering out high frequency noise:
\begin{align}
% symmetrical median filter
f_1(x^t_i) &= x^{t-1}_i + x^t_i + x^{t+1}_i, \label{eq:symetric-median} \\
% asymmetrical weighted median
f_2(x^t_i) &= 0.4 x^{t-3}_i + 0.6 x^{t-2}_i + 0.8 x^{t-1}_i + x_i^t.
\label{eq:weighted-asymetric-median}
\end{align}
\DIFaddbegin \DIFadd{These filters are standard in the signal processing field \mbox{%DIFAUXCMD
\citep{kaiser1977data, oppenheim1983signals}
}%DIFAUXCMD
. The idea of the second one is to not consider future values and to introduce decreasing weights. }\DIFaddend For the sake of illustration, the effect of the filter $f_1$ on the signal
is shown in Figure \ref{fig:lp_curve}.

Furthermore, \DIFdelbegin \DIFdel{it can be observed that neurons communicate through }\DIFdelend short spikes, characterized by a high
frequency, \DIFaddbegin \DIFadd{can be seen as an indirect indicator of neuron communications }\DIFaddend while low frequencies of the signal mainly correspond to the slow
decay of fluorescence. To have a signal that only has high magnitude around instances where the spikes occur, the second step of our pipeline transforms the time series into its backward
difference
\begin{align}
g(x^{t}_{i}) &= x^{t}_i - x^{t-1}_i, \label{eq:high-pass-filter}
\end{align}
as shown in Figure \ref{fig:hp_curve}.

To filter out small variations in the signal obtained after applying the
function $g$ as well as to eliminate negative values, we use the following
hard-threshold filter
\begin{align}\label{eqn:hfilter}
h(x^{t}_i) &= x^{t}_i \mathbb{1}(x^{t}_i \geq \tau) \text{ with } \tau > 0,
\end{align}
yielding Figure \ref{fig:threshold_curve} \DIFaddbegin \DIFadd{where $\tau$ is the threshold parameter and $\mathbb{1}$ is the indicator function}\DIFaddend .
As we can see, the processed signal only contains clean spikes.

The objective of the last step of our filtering procedure is to decrease the
importance of spikes that occur when there is a high global activity in the
network with respect to  spikes that occur during normal activity. Indeed, we
have conjectured that when a large part of the network is firing, the rate at
which observations are made is not high enough for being able to detect
interactions, and that it would therefore be preferable to lower their
importance by changing appropriately their magnitude. Additionally, it is
well-known that neurons may also spike because of a global high activity
\citep{stetter2012model}. In such  context, detecting pairwise neuron
interactions from the firing activity is meaningless. As such,
the signal output by $h$ is finally applied to the following function
\begin{align}
 w(x^{t}_i) &= (x^{t}_i + 1 )^{1 + \frac{1}{\sum_{j} x^{t}_j}}, \label{eq:magnify-filter}
\end{align}
whose effect is to magnify the importance of spikes that occur in case of low
global activity (measured by $\sum_{j} x^{t}_j$), as observed for instance
around $t=4\text{s}$ in Figure~\ref{fig:weight_curve}. Note that the particular case where there
is no activity, i.e., $\sum_{j} x^{t}_j = 0$, is solved by setting $w(x^{t}_i)
= 1$.

To summarize, the full signal processing pipeline of our simplified approach is defined by the composed function $w \circ h \circ g \circ
f_1$ (resp. $f_2$). When applied to the raw signal of Figure
\ref{fig:original_curve}, it outputs the signal shown in Figure
\ref{fig:weight_curve}.


\section{Connectome inference from partial correlation statistics}
\label{sec:inference}

% No time assumption

Our procedure to infer connections between neurons first assumes that
the (filtered) fluorescence concentrations of all $p$ neurons at each
time point can be modeled as a set of random variables $X = \{X_1,
\dots, X_p\}$ that are independently drawn from the same time invariant
joint probability distribution $P_X$. %% Or, say otherwise, we assume
%% that there exists a probability distribution $P_X$ such that the
%% fluorescence concentrations at a particular time $t$ can be seen as an
%% independent realization of $P_X$.
As a consequence, our inference method does not exploit the time
ordering of the observations (although time ordering is exploited by
the filters).

% partial correlation: definition + motivation

Given this assumption, we then propose to use as a measure of the
strength of the connection between two neurons $i$ and $j$, the
\textit{Partial correlation} coefficient $p_{i,j}$ between their corresponding
random variables $X_i$ and $X_j$, defined by:
\begin{equation}
p_{i,j} =
-\frac{\Sigma^{-1}_{ij}}{\sqrt{\Sigma^{-1}_{ii} \Sigma^{-1}_{jj}}}, \label{eq:inverse}
\end{equation}
where $\Sigma^{-1}$, known as the precision or concentration matrix, is the inverse of the covariance matrix $\Sigma$ of $X$. %%  Partial
%% correlation can be interpreted in several ways. The partial
%% correlation coefficient $p_{i,j}$ can be shown to measure the
%% correlation between $X_i$ and $X_j$ when they are corrected for the
%% (linear) effect of all other variables in $X$ \cite{}.
Assuming that the distribution $P_X$ is a multivariate Gaussian
distribution ${\cal N}(\mu,\Sigma)$, it can be shown that $p_{i,j}$ is
zero if and only if $X_i$ and $X_j$ are independent given all other
variables in $X$, i.e., $X_i \perp X_j|X^{-i,j}$ where $X^{-i,j}= X
\setminus\{X_i,X_j\}$. Partial correlation thus measures conditional
dependencies between variables. It should thus naturally only detect direct associations
between neurons and filter out spurious indirect effects. The interest
of partial correlation as an association measure has already been
shown for the inference of gene regulatory networks
\citep{de2004discovery,Schafer:2005}.
%% Given a set of multivariate Gaussian
%% random variables $X$, the (undirected) graph that connects all pairs
%% of variables $X_i$ and $X_j$ such that $p_{i,j}\neq 0$ is called a
%% Gaussian graphical models (GGMs). Because it measures conditional
%% dependencies, partial correlation should naturally only detect direct
%% associations between neurons. The superiority of partial correlation
%% over standard correlation as an association measure have already been
%% shown in the context of the inference of gene regulatory networks from
%% gene expression data \cite{}. We will compare both measures in Section
%% \ref{sec:results}.
Note that the partial correlation statistic is symmetric
(i.e. $p_{i,j}=p_{j,i}$). Therefore, our approach cannot identify the
direction of the interactions between neurons. We will see in
Section~\ref{sec:results} why this only affects slightly its
performance, with respect to the metric used in the Connectomics
challenge.

Practically, the computation of all $p_{i,j}$ coefficients using Equation
\ref{eq:inverse} requires the estimation of the covariance matrix $\Sigma$
and then computing its inverse. Given that we have typically more
samples than neurons, the covariance matrix can be inverted in a
straightforward way. We nevertheless obtained some improvement by
replacing the exact inverse with an approximation using only the $M$
first principal components \citep{bishop2006pattern} (with
$M=0.8 p$ in our experiments\DIFaddbegin \DIFadd{, see Appendix~\ref{app:pca}}\DIFaddend ).

Finally, let us note that the performance of our simple method appears to
be quite sensitive to the values of parameters (e.g., choice of $f_1$ or $f_2$
or the value of the threshold $\tau$) in the combined function of the
filtering and inferring processes. One approach, further referred
to as \textit{Averaged Partial correlation} statistics, for improving
its robustness is to average correlation statistics over various
values of the parameters, thereby reducing the variance of its
predictions. Further details about parameter selection are provided in
Appendix~\ref{app:optimized}.


%DIF <  Note on directionality
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < % Our  connection inference  procedure  relies  on several  assumptions. First,
%DIF < % we will assume that  the fluorescence concentrations of all $p$ neurons can be
%DIF < % modeled as a set of random variables $X = \{X_1, \dots, X_p\}$  that are drawn
%DIF < % from a time invariant joint  probability distribution $P_X$. Or, say otherwise,
%DIF < % we assume that there exists a probability distribution $P_X$ such that the
%DIF < % fluorescence concentrations at a particular time $t$ can be seen as an
%DIF < % independent realization of $P_X$.
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < % To assess the presence of a connection between two neurons, we will
%DIF < % use partial correlation statistics, which has been used extensively
%DIF < % for the inference of gene regulatory networks \cite{}.
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < % To a joint probability distribution $P_X$ corresponds a
%DIF < % graph where (i) each node is a random variable and (ii) two nodes are
%DIF < % only connected if they correspond to two dependent variables. We assume that two neurons are
%DIF < % connected if  and only if,  there exists  in this  graph an edge
%DIF < % that connects  directly the  two random  variables to  which they correspond.
%DIF < % In this context, it is equivalent to writing that two neurons $i$ and $j$ are  not  directly
%DIF < % connected  if and only if  $X_i$  and $X_j$  are  conditionally independent
%DIF < % with  respect to all  other variables. We remind  that two random  variables
%DIF < % $X_i$  and  $X_j$  are  said  to  be  conditionally independent to a (possibly
%DIF < % empty)  set of controlling random variables $Z$, denoted $X_i \perp X_j | Z$,
%DIF < % if the joint conditional probability distribution   $P_{X_i,  X_j   |   Z}$
%DIF < % factorizes  into   $P_{X_i|Z} P_{X_j|Z}$.  In particular,  when  $X_i$ and
%DIF < % $X_j$ are  conditionally independent  with respect  to all  the other
%DIF < % variables, denoted  $X_i \perp  X_j  | X^{-i,j}$,  then  the  observation  of
%DIF < % $X_i$  given  the knowledge  of  $X^{-i,j}$ provides  no information  about
%DIF < % $X_j$,  and vice-versa.
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < % In the  particular case where $P_X$ is  a joint  Gaussian distribution
%DIF < % $\mathcal{N}(\mu; \Sigma)$ of mean  vector $\mu$ and covariance matrix
%DIF < % $\Sigma$, $X_i$  and $X_j$ are conditionally  independent with respect to the
%DIF < % other variables $X^{-i,j}$ if their \textit{partial correlation} statistic is
%DIF < % null~\citep{baba2004partial}, i.e., if
%DIF < % \begin{equation}
%DIF < % \rho_{X_i, X_j | X^{-i,j}} = -\frac{\Sigma^{-1}_{ij}}{\sqrt{\Sigma^{-1}_{ii} \Sigma^{-1}_{jj}}} = 0, \label{eq:inverse}
%DIF < % \end{equation}
%DIF < % where $\Sigma^{-1}$ is known as the precision or concentration matrix.
%DIF < % Conversely, $X_i$ and $X_j$ are conditionally dependent if $\rho_{X_i, X_j |
%DIF < % X^{-i,j}} \neq 0$, where $\rho_{X_i, X_j | X^{-i,j}} \in [-1;1]$. Note that the
%DIF < % larger the value of this term, the stronger the dependence between variables
%DIF < % $X_i$ and $X_j$.
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < % Our approach implicitly makes the additional assumption  that $P_X$ is  a
%DIF < % joint Gaussian distribution since it  outputs as degree of association
%DIF < % between two neurons $i$ and $j$ an estimate of $\rho_{X_i, X_j | X^{-i,j}}$.
%DIF < % Note also that the partial correlation
%DIF < % statistic is symmetric  (i.e., $\rho_{X_i, X_j | X^{-i,j}} = \rho_{X_j, X_i |
%DIF < % X^{-j,i}}$). Therefore, our approach cannot identify the direction of the interaction. We will see in Section~\ref{sec:results} why this only affects slightly its performances, with respect to the metric used in the Connectomics challenge.
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < % We now detail our procedure for estimating partial correlation
%DIF < % statistics ($\rho_{X_i, X_j | X^{-i,j}}$) from the data. It first
%DIF < % estimates the covariance matrix from the data. Afterwards, it computes
%DIF < % the exact inverse of this matrix, from which it infers the value of
%DIF < % $\rho_{X_i, X_j | X^{-i,j}}$ using equation (\ref{eq:inverse}). Note
%DIF < % that we have also experimented procedures where the exact inverse was
%DIF < % replaced by an approximation of it using only the $M$ principal
%DIF < % components of $\Sigma$~\citep{bishop2006pattern} whose associated
%DIF < % eigen values were significantly higher than zero. Such an
%DIF < % approximation is equivalent to assume that the firing activity of all
%DIF < % neurons can be explained by only a subset of them whose size is equal
%DIF < % to $M$. We observed that those procedures were often leading to better
%DIF < % results. Note that we found out by running expriments that the number
%DIF < % $M$ of principal components is around $0.8p$ (see discussion in
%DIF < % Appendix~\ref{app:REF} \textcolor{red}{REF}).
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < % % Computationally, they can be estimated
%DIF < % % % efficiently in $O(M^3)$ (instead of $O(p^3)$)
%DIF < % % from the $M$ first principal
%DIF < % % components extracted through PCA~\citep{bishop2006pattern} on the processed
%DIF < % % data. While speeding up computation, it allows to further
%DIF < % % filter the the data by removing the contribution of components that are respectively
%DIF < % % due to redundancy or noise, i.e., with an eigen value respectively is equal to zero or close to zero.
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \section{Experiments} \label{sec:results}


%DIF < % \textcolor{red}{to incorporate:
%DIF < % To highlight the benefits of measuring the partial dependence conditional to
%DIF < % all other variables, we compare in Section~\ref{sec:results} with the
%DIF < % \textit{Pearson correlation} statistics, which measure instead the linear
%DIF < % dependence between variables, independently of the others. Accordingly, $X_i$
%DIF < % and $X_j$ are independent, without any conditioning, if their Pearson correlation statistic is zero, that
%DIF < % is if
%DIF < % \begin{equation}
%DIF < % \rho_{X_i,X_j} = \frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii} \Sigma_{jj}}} = 0.
%DIF < % \end{equation}}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \paragraph{Data and evaluation metrics.}

We report here experiments on the \textit{normal-1,2,3}, and \textit{4}
datasets provided by the organizers of the Connectomics challenge (see
Appendix~\ref{app:supp} for experiments on other datasets). Each of
these datasets is obtained from the simulation \citep{stetter2012model} of
different neural networks of 1000 neurons and about 15000 edges each (i.e., a
network density of about 1.5\%). Each neuron is described by a calcium
fluorescence time series of length $T=179500$. All inference methods compared
here provide a ranking of all pairs of neurons according to some association score. To assess the quality of this ranking, we compute both ROC and
precision-recall curves against the ground truth network, which are summarized
using the area under the curve and respectively  denoted AUROC and AUPRC. Only
the AUROC score was used to rank the challenge participants but the precision-recall curve has been shown to be a more sensible metric for network
inference, especially when network density is small (see e.g.,
\cite{schrynemackers2013protocols}). Since neurons are not self-connected in
the ground truth networks (i.e., $(i, i) \not \in E, \forall i \in V$), we
have manually set the score of such edges to the minimal possible association
score before computing ROC and PR curves.

\paragraph{Evaluation of the method.}

The top of Table \ref{tab:comparison} reports AUROC and AUPRC for all
four networks using in each case partial correlation with different
filtering functions. Except for the last two rows that use PCA, the
exact inverse of the covariance matrix was used in each case. These
results clearly show the importance of the filters. AUROC increases in
average from 0.77 to 0.93. PCA does not really affect AUROC scores but
it significantly improves AUPRC scores. Taking the average over
various parameter settings gives an improvement of 10\% in AUPRC but
only a minor change in AUROC. The last row (``Full method'') shows the
final performance of the method specifically tuned for the challenge
(see Appendix \ref{app:optimized} for all details). Although this
tuning was decisive to obtain the best performance at the challenge,
it does not significantly improve neither AUROC nor AUPRC.

\begin{table}[t]
%\caption{Evaluation of different methods on the four \textit{normal} datasets\label{tab:comparison}}
\caption{Top: Performance on \textit{normal-1,2,3,4} with partial correlation and different filtering functions.
Bottom: Performance on \textit{normal-1,2,3,4} with different methods.}
\label{tab:comparison}
\centering
\small
\begin{tabular}{| l | c c c c | c c c c |}
\hline
& \multicolumn{4}{c|}{AUROC} & \multicolumn{4}{c|}{AUPRC} \\
\textit{Method} $\backslash$ \textit{normal-} & \textit{1} & \textit{2} & \textit{3} & \textit{4} & \textit{1} & \textit{2} & \textit{3} & \textit{4} \\
\hline
\hline
No  filtering       					& 0.777 & 0.767 & 0.772 & 0.774 & 0.070 & 0.064 & 0.068 & 0.072\\
$ h \circ g \circ f_1$                  & 0.923 & 0.925 & 0.923 & 0.922 & 0.311 & 0.315 & 0.313 & 0.304\\
$ w \circ h \circ g \circ f_1$          & 0.931 & 0.929 & 0.928 & 0.926 & 0.326 & 0.323 & 0.319 & 0.303\\
+ PCA         							& 0.932 & 0.930 & 0.928 & 0.926 & 0.355 & 0.353 & 0.350 & 0.333\\
Averaging           					& 0.937 & 0.935 & 0.935 & 0.931 & 0.391 &  0.390 &  0.385 & 0.375\\
Full method           					& \textbf{0.943} & \textbf{0.942} & \textbf{0.942} & \textbf{0.939} & \textbf{0.403} & \textbf{0.404} & \textbf{0.398} & \textbf{0.388}\\
\hline
PC & 0.886 & 0.884 & 0.891 &  0.877 & 0.153 & 0.145 & 0.170 & 0.132\\
GTE & 0.890 & 0.893 & 0.894 & 0.873 & 0.171 & 0.174 & 0.197 & 0.142\\
GENIE3 & 0.892 & 0.891 & 0.887 & 0.887 & 0.232 & 0.221 & 0.237 & 0.215 \\
\hline
\end{tabular}

\end{table}

\paragraph{Comparison with other methods.}

In the bottom of Table \ref{tab:comparison}, we provide as a comparison the
performance of three other methods: standard (Pearson) correlation (PC),
generalized transfer entropy (GTE), and GENIE3. ROC and PR curves on the
\textit{normal-2} network are shown for all methods in Figure
\ref{fig:curves}. Pearson correlation measures the unconditional linear
(in)dependence between variables and it should thus not be able to filter out
indirect interactions between neurons. GTE \citep{stetter2012model} was
proposed as a baseline for the challenge. This method builds on Transfer
Entropy to measure the association between two neurons. Unlike our approach, it
can predict the direction of the edges. GENIE3 \citep{huynhthu2010inferring} is
a gene regulatory network inference method that was the best performer of the
DREAM5 challenge \citep{marbach2012}. When transposed to neural networks, this
method uses as a confidence score for the edge going from neuron $i$ to neuron
$j$ the importance score of variable $X_i$ in a Random Forest model trying to
predict $X_j$ from all variables in $X\setminus X_j$. To reduce the
computational cost of this method, we had however to limit each tree in the
Random Forest model to a maximum depth of 3. This constraint potentially
severely affects the performance of this method with respect to the use of
fully grown trees. PC and GENIE3 were applied on the time series filtered using the functions $w\circ h\circ g$ and $h\circ g\circ f_1$ (which
gave the best performance), respectively. For GENIE3, we built 10000 trees per neuron and we
used default settings for all other parameters (except for the maximal tree
depth). For GTE, we reproduced the exact same setting (conditioning level and
pre-processing) that was used by the organizers of the challenge.

%% To assess our method, we focus on the \textit{normal-2} dataset
%% provided by the organizers of the Connectomics challenge. Experiments
%% on \textit{normal-1, normal-3, normal-4} show similar trends to
%% \textit{normal-2} and are presented in the Appendix \textcolor{red}{REF}.
%% Those datasets have $p=1000$ time series of length $T=179500$ of image
%% calcium from simulated neuronal networks \citep{stetter2012model}. Each of these
%% networks contains approximately 15000 edges.

%% % Metrics : ROC AUC and AUPRC
%% In supervised learning terms, the network inference task can be viewed as a
%% binary classification task where one has to correctly classify the presence or
%% the absence of edges of the graph. We assess the accuracy of the network
%% inference method using the area under the ROC curve (AUROC) and the area under
%% the precision-recall curve (AUPRC) \citep{schrynemackers2013protocols}.


%% The proposed approach is compared to Pearson correlation statistics
%% (as described in Section~\ref{sec:inference}), generalized transfer
%% entropy (GTE) and the GENIE3 algorithm.
%% \begin{itemize}
%% \item \textbf{GTE} \citep{stetter2012model}, the baseline of the challenge,
%% measures for all pairs of neurons $i$ and $j$ the reduction of uncertainty in
%% predicting future values of $i$ given the observation of the time series of the
%% neuron $j$ conditionally to the observation of past time series of neuron $i$,
%% whenever the average fluorescence is over a conditioning level. Pre-processing
%% and conditioning level were set to reproduce baseline of the challenge.

%% \item \textbf{GENIE3} \citep{huynhthu2010inferring} translates the inference
%% of the network of $p$ neurons into $p$ supervised learning tasks. For each
%% task, the fluorescence of a neuron is predicted from the fluorescence of all
%% other neurons using a tree based ensemble such as random forest
%% \citep{breiman2001random}. The tree ensemble associates a variable importance
%% score \citep{louppe2013understanding} to each neuron used in the predictions,
%% which is used as a degree of associate. The following parameters are used
%% $10000$ trees, $\log_2{p}$ variables are randomly selected to split tree nodes
%% and the maximal tree depth is constrained to 1, 2 or 3. \textcolor{red}{AJ Which
%% depth is represented on the graph?}
%% \end{itemize}

%% \subsection*{Partial correlation improves over state-of-the-art methods}

Partial correlation and averaged partial correlation clearly outperform all
other methods on all datasets (see Table \ref{tab:comparison} and Appendix \ref{app:supp}). The
improvement is more important in terms of AUPRC than in terms of AUROC. As
expected, Pearson correlation performs very poorly in terms of AUPRC. GTE and
GENIE3 work much better but these two methods are nevertheless clearly below
partial correlation. Among these two methods, GTE is slightly better in terms
of AUROC, while GENIE3 is significantly better in terms of AUPRC. Given that we
had to limit this latter method for computational reasons, these results are
very promising and a comparison with the full GENIE3 approach is certainly part
of our future works.

The fact that our method is unable to predict edge directions does not seem to
be a disadvantage with respect to GTE and GENIE3. Although partial correlation
scores each edge and its opposite similarly, it can reach precision values
higher than 0.5 (see Figure \ref{fig:curves}(b)), suggesting that it mainly ranks high
pairs of neurons that interact in both directions.  It is interesting also to
note that, on \textit{normal-2}, a method that perfectly predicts the
undirected network (i.e., that gives a score of $1$ to each pair $(i,j)$ such that
$(i,j)\in E$ or $(j,i)\in E$, and $0$ otherwise) already reaches an AUROC as high
as $0.995$ and an AUPRC of $0.789$.


%% bu by about a factor
%% of 2 in AUPRC reaching $0.347$ and an increased of $0.03$ in
%% AUROC. Averaging Partial correlation matrix over a wide of parameters
%% further improves AUPRC to $0.389$ and improve AUROC by $0.009$.  Two
%% reasons might explain such differences: (i) only the partial
%% correlation-based methods and GENI3 at lesser extent distinguish
%% direct links from indirect one, (ii) simulated networks from the
%% \citep{stetter2012model} generator or neural networks in general could
%% be reduced to a linear model after pre-processing. Those results
%% generalizes over the other datasets see Appendix \textcolor{red}{REF}.

\begin{figure}[t]
\centering
\subfigure[ROC curves]{\includegraphics[width=0.45\textwidth]{curve_roc} \label{fig:roc_curve}}
\subfigure[Precision-recall curves]{\includegraphics[width=0.45\textwidth]{curve_pr} \label{fig:pr_curve}}
\caption{ROC (left) and PR (right) curves on \textit{normal-2} for the compared methods. Areas under the curves are reported in the legend.}
%% The weighted average of partial correlation matrix, winning approach
         %% of the challenge, yields better than AUROC and AUPRC than all
         %% other state-of-the-art methods. (parameters: $\tau=0.11$ except for
         %% averaged partial correlation method, partial correlation used
         %% filter $f_1$). Area under the curves are given between $()$
         %% for the corresponding methods.
%}
\label{fig:curves}
\end{figure}

%% \textcolor{red}{Only one GENI3, thus it can't be observed.}
%% As observed on Figure \ref{fig:curves}, the GENIE3 performance increases with
%% deeper trees while being still computationally feasible. The reason is twofold.
%% First, the deeper the tree, the larger is the conditioning. \textcolor{red}{AJ:
%% conditioning?} As it was observed in the comparison between correlation and
%% partial correlation, it screens indirect links in the inferred network.
%% \textcolor{red}{AJ: I don't understand the link with GENIE3} Second, the number
%% of small trees need to be much higher than deep trees in order to observe - and
%% evaluate - all pairs.  However, as suggested by \cite{louppe2014understanding},
%% limiting the maximal depth of trees may help to improve the variable importance
%% computing, i.e. inferring the degree of association. \textcolor{red}{AJ: there
%% is no explanation of why it would be better apart for computational reason.}
%% Therefore a trade-off  of the maximal depth may give the best of both worlds.
%% \textcolor{red}{AJ:  Which tradeoff? Which worlds?}


%\subsection*{ROC versus PR curves.}

%% The best method only infers the \textbf{undirected} network by giving the same
%% degree of association to both edges $(i,j)$ and $(j,i)$. This is quite
%% surprising considering the problem statement. Trying to direct links could be
%% more penalizing than rewarding. For example, symmetrizing the degree of
%% association matrix does not necessarily degrade AUPRC and AUROC.


%% If the inferred network corresponds to the undirected network, it would
%% already yield high AUROC score for instance $\approx 0.996$ on
%% \textit{normal-1}. Infering edge directivity is likely to be counter-
%% productive or yield small gain. By contrast, the same experiment with AUPRC
%% would yield a score of $\approx 0.789$ on \textit{normal-1}. Thus inferring
%% correctly directivity would be more rewarding in AUPRC term.

%% % Interestingly, methods that takes into account directivity, namely GTE and
%% % GENIE3, improve their AUROC and AUPRC by giving the same way to directed edges.
%% % (in particular see GTE and GTE(undirected) in Figure \ref{fig:pr_curve}
%% % \textcolor{red}{AJ: Figure \ref{fig:curves}  contradicts this message,
%% % especially pr-curve. AS:And now?} and Table \ref{tab:tab1} in Appendix \textcolor{red}{REF}).
%% % Trying to detect the link direction is more penalizing than rewarding.

%% The area under the ROC curve is not a consistent metric with respect to the
%% problem statement. A clear hierarchy is also difficult to derive from ROC
%% curves as shown on Figure \ref{fig:roc_curve}. We advice to avoid the AUROC for
%% the assessment of network inference method. An alternative would be to assess
%% network inference using the area under the precision-recall curve (AUPRC).
%% Those conclusions are in line with \cite{schrynemackers2013protocols}.


\section{Conclusions} \label{sec:conclusion}

In this paper, we outlined a simple but efficient methodology for the problem
of connectome inference from calcium imaging data. Our approach consists of two
steps: (i) processing fluorescence data to detect neural peak activities and
(ii) inferring the degree of association between neurons from partial
correlation statistics. Its simplified variant outperforms other
network inference methods while its optimized version proved to be the best method
on the Connectomics challenge. Given its simplicity and good performance, we
therefore believe that the methodology presented in this work
would constitute a solid and easily reproducible baseline for further work in
the field of connectome inference.


\paragraph{Acknowledgments}
A. Joly and G. Louppe are research fellows of the FNRS, Belgium.  A. Sutera is
recipient of a FRIA fellowship of FRS-FNRS, Belgium. This work is supported by
PASCAL2 and the IUAP DYSCO, initiated by the Belgian State, Science Policy
Office.



\newpage
\clearpage

% We allow the bibliography to be added on top of the 6 pages.
% Supplementary material may be added in the form or a URL pointed to from
% the paper.

\bibliography{references}

\newpage
\clearpage

\appendix


\section{Description  of the ``Full method''}
\label{app:optimized}

This section provides a detailed description of the method specifically tuned
for the Connectomics challenge. We restrict our description to the
differences with respect to the simplified method presented in the main
paper. Most parameters were tuned so as to maximize AUROC on the
\textit{normal-1} dataset and our design choices were validated by monitoring
the AUROC obtained by the 145 entries we submitted during the
challenge. Although the tuned method performs better than the simplified one on
the challenge dataset, we believe that the tuned method clearly overfits the
simulator used to generate the challenge data and that the simplified method
should work equally well on new independent datasets. We nevertheless provide
the tuned method here for reference purpose. Our implementation of the tuned
method is available at \url{https://github.com/asutera/kaggle-connectomics}.

This appendix is structured as follows: Section~\ref{sapp:signal} describes
the differences in terms of signal processing. Section~\ref{sapp:averaging}
then provides a detailed presentation of the averaging approach.
Section~\ref{sapp:connectome} presents an approach to correct the $p_{i,j}$
values so as to take into account the edge directionality. Finally,
Section~\ref{sapp:results} presents some experimental results to validate the
different steps of our proposal.

% Let us remark that tuning
% was mainly done on the \textit{normal-1} and \textit{normal-4}
% datasets, which explains why these two networks are slightly better
% retrieved than the others.

\subsection{Signal processing}
\label{sapp:signal}

In Section~\ref{sec:filter}, we introduced four filtering functions ($f$, $g$,
$h$, and $w$) that are composed in sequence (i.e., $w \circ h \circ g \circ
f$) to provide the signals from which to compute partial correlation
statistics. Filtering is modified as follows in the tuned method:

\begin{itemize}
\item In addition to $f_1$ and $f_2$ (Equations \ref{eq:symetric-median} and
  \ref{eq:weighted-asymetric-median}), two alternative low-pass filters $f_3$
  and $f_4$ are considered:
\begin{align}
f_3(x^t_i) &= x^{t-1}_i + x^{t}_i + x^{t+1}_i + x^{t+2}_i, \label{eq:asymetric-median-forward} \\
f_4(x^t_i) &=  x_i^t + x^{t+1}_i  + x^{t+2}_i + x^{t+3}_i. \label{eq:asymetric-median}
\end{align}
\item An additional filter $r$ is applied to smoothen differences in peak magnitudes
  that might remain after the application of the hard-threshold filter $h$:
\begin{align}
r(x^t_i) = (x_i^t)^c,
\end{align}
with $c=0.9$.
\item Filter $w$ is replaced by a more complex filter $w^*$ defined as:
\begin{align}
 w^*(x^{t}_i) &= {(x^{t}_i + 1 )^{\left (1 + \frac{1}{\sum_{j} x^{t}_j}\right )}}^{k(\sum_{j} x^{t}_j)}
\end{align}
where the function $k$ is a piecewise linear function optimized separately for
each filter $f_1$, $f_2$, $f_3$ and $f_4$ (see the implementation for full
details). Filter $w$ in the simplified method is a special case of $w^*$ with
$k(\sum_j x_j^t)=1$.
\end{itemize}
The pre-processed time-series are then obtained by the application of the
following function: $w^*\circ r \circ h \circ g \circ f_i$ (with $i=1$, 2, 3, or 4).

%% two composed filtering functions in
%% order (i) to remove noise induced by light scattering artifact, (ii) to take
%% into account the slow decay of fluorescence signal and (iii) to reduce the
%% importance of global high activity in the network. However, in order to win the
%% challenge, we have heavily optimized several different, but yet similar,
%% filtering composed function. On the one hand, each part of the composed
%% functions is optimized to maximize the ROC AUC (e.g., $w^*$). On the other
%% hand, we introduce new functions (e.g., $f_3$ and $f_4$) leading to alternative
%% composed functions. They might lead to worse ROC AUC when taken individually
%% but they improve the overall ROC AUC when taking as a part of the averaging
%% approach.

% Taken individually, those composed functions in
% conjunction with partial correlation might lead to worse AUROC, but improved
% whenever they are averaged.

%% Besides filters $f_1$ and $f_2$ respectively defined by equations \ref{eq:symetric-median}
%% and \ref{eq:weighted-asymetric-median}, we also consider two
%% other low-pass filters
%% \begin{align}
%% f_3(x^t_i) &= x^{t-1}_i + x^{t}_i + x^{t+1}_i + x^{t+2}_i, \label{eq:asymetric-median-forward} \\
%% f_4(x^t_i) &=  x_i^t + x^{t+1}_i  + x^{t+2}_i + x^{t+3}_i. \label{eq:asymetric-median}
%% \end{align}

%% After the hard thresholded filter $h$, they were significant differences in
%% peak magnitudes. Since time series $x_i^t$  were mainly between 0 and 1,
%% we smoothened these differences in peak magnitudes through
%% \begin{align}
%% p(x^t_i) = (x_i^t)^c
%% \end{align}
%% yielding slightly better AUROC with $c=0.9 \in (0, 1)$.

%% After filter $p$, we applied a filter $w^*$ similar to $w$, whose goal is also
%% to magnify the importance of spikes that occur in case of low global activity.
%% Instead of equation \ref{eq:magnify-filter}, we had
%% \begin{align}
%%  w^*(x^{t}_i) &= {(x^{t}_i + 1 )^{\left (1 + \frac{1}{\sum_{j} x^{t}_j}\right )}}^{k(\sum_{j} x^{t}_j)}
%% \end{align}
%% where the function $k$ is a piecewise linear function optimized for each filter
%% $f_1$, $f_2$, $f_3$ and $f_4$. By comparison, the filter $w$ used the following
%% function $k(\sum_{j} x^{t}_j) = 1$. For more details on this aspect, we refer
%% the reader to the implementation \footnote{available on
%% \url{https://github.com/asutera/kaggle- connectomics}.}.

\subsection{Weighted average of partial correlation statistics}
\label{sapp:averaging}

As discussed in Section \ref{sec:inference}, the performance of the method (in
terms of AUROC) is sensitive to the value of the parameter $\tau$ of the
hard-threshold filter $h$ (see Equation \ref{eqn:hfilter}), and to the choice
of the low-pass filter (among $\{f_1, f_2, f_3, f_4\}$).
Like in the simplified method, we have averaged the partial correlation statistics obtained for all the pairs $(\tau,\mbox{low-pass filter}) \in \{0.100,0.101,\ldots,0.209\}\times \{f_1, f_2, f_3, f_4\}$.

Filters $f_1$ and $f_2$ display similar performances and thus were given similar
weights (i.e., resp. $0.383$ and $0.345$). These weights were chosen equal to the weights selected for the simplified method. In contrast, filters $f_3$
and $f_4$ turn out to be individually less competitive and were therefore given
smaller importance in the weighted average (i.e., resp. $0.004$ and $0.268$). Yet, as further shown in
Section~\ref{sapp:results}, combining all $4$ filters proves to slightly
improve performance with respect to using $f_1$ and $f_2$ only.

%% Given the unequal AUROC performance of $f_1$, $f_2$, $f_3$ and $f_4$, the
%% contribution of each filter in the degree of association between two neurons
%% was chosen as to maximize the AUROC on \textit{normal-1} and is function of
%% their individual performance. By comparison in the simple method, each filter
%% $f_1$ and $f_2$ were given equal weight.


\subsection{Prediction of edge orientation}
\label{sapp:connectome}

Partial  correlation  statistics is  a  symmetric  measure, while  the
connectome is a directed graph. It  could thus be beneficial to try to
predict edge orientation. In this section, we present a heuristic that
modifies the  $p_{ij}$ computed  by the  approach described  before to
take into account directionality.

This approach is based on the following
observation. The rise of fluorescence of a neuron indicates its
activation. If another neuron is activated after a slight delay, this
could be a consequence of the activation of the first neuron and
therefore indicates a directed link in the connectome from the first to
the second neuron.  Given this observation, we have computed for every
pair $(i,j)$ the term:
\begin{align}
s_{i,j} = \sum_{t=1}^{T - 1} \mathbb{1}((x_j^{t+1} - x_i^t) \in \left[\phi_1, \phi_2\right])
\end{align}
that could be interpreted as an image of the  number of times
that neuron $i$ activates neuron $j$. $\phi_1$ and $\phi_2$ are
parameters whose values have been chosen in our experiments equal to
$0.2$ and $0.5$, respectively. Their role is to
define when the difference between $x_j^{t+1}$  and $x_i^t$ can
indeed be assimilated to an event for which neuron $i$ activates neuron
$j$.

Afterwards, we have computed the difference between $s_{i,j}$ and
$s_{j,i}$, that we call $z_{i,j}$, and used this difference to modify  $p_{i,j}$ and
$p_{j,i}$ so as to take into account directionality. Naturally, if
$z_{i,j}$ is greater  (smaller) than $0$, we may conclude that should there  be an
edge between $i$ and $j$, then this edge would have to be oriented
from $i$ to $j$ ($j$ to $i$).

This suggests the new association matrix $r$:
\begin{align}
r_{i,j} =  \mathbb{1}(z_{i,j} > \phi_3)  *  p_{i,j}
\end{align}
where $\phi_3 >0$ is another parameter. We found out that this new
matrix $r$ was not giving good results, probably due to the fact that
directivity was not well enough rewarded in the challenge.

This has lead us to investigate other ways for exploiting the
information about directionality contained in the matrix $z$. One of
those ways that gave good performance was to use as association
matrix:
\begin{align}
q_{i,j} = weight * p_{i,j} + (1-weight) * z_{i,j}
\label{eqn:qij}
\end{align}
with  $weight$ chosen close to 1 ($weight=0.997$). Note that with
values for $weight$ close to 1,   matrix $q$ only slightly uses the
information about directivity contained in $z$ to modify the  partial
correlation matrix $p$. We tried smaller values for $weight$ but those
were leading to worse results.

It was  this association matrix $q_{i,j}$ that actually led to the
best results of the challenge, as shown in Table \ref{tab:directivity}
of Section~\ref{sapp:results}.

\subsection{Experiments}
\label{sapp:results}

\paragraph{On the interest of low-pass filters $f_3$ and $f_4$.}

%%% SHOW DIFFERENTS COMPOSED FUNCTION


% Theses filters lead, taken individually, to worse performances  than filters
% $f_1$ and $f_2$ but averaging the results of all low-pass filters leads to better ROC AUC.

As reported in Table~\ref{tab:f3f4}, averaging over all low-pass filters leads
to better AUROC scores than averaging over only two low-pass filters, i.e., $f_1$ and
$f_2$. It however slightly deteriorates AUPRC.

\begin{table}[ht]
%\caption{Evaluation of different methods on the four \textit{normal} datasets\label{tab:comparison}}
\caption{Performance on \textit{normal-1, 2, 3, or 4} with partial correlation with different averaging approaches.}
\label{tab:f3f4}
\centering
\small
\begin{tabular}{| l | c c c c | c c c c |}
\hline
& \multicolumn{4}{c|}{AUROC} & \multicolumn{4}{c|}{AUPRC} \\
\textit{Averaging} $\backslash$ \textit{normal-} & \textit{1} & \textit{2} & \textit{3} & \textit{4} & \textit{1} & \textit{2} & \textit{3} & \textit{4} \\
\hline
\hline
 with $f_1$, $f_2$ & 0.937 & 0.935 & 0.935 & 0.931 & 0.391 &  \textbf{0.390} &  0.385 & \textbf{0.375}  \\
 with $f_1$, $f_2$, $f_3$, $f_4$ & \textbf{0.938} & \textbf{0.936} & \textbf{0.936} & \textbf{0.932} & 0.391 & 0.389 & 0.385 & 0.374\\
\hline
\end{tabular}
\end{table}

\paragraph{On the interest of using matrix $q$ rather than $p$ to take into account directivity.}

Table~\ref{tab:dir} compares AUROC and AUPRC with or without correcting the $p_{i,j}$ values according to Equation \ref{eqn:qij}. Both AUROC and AUPRC are (very slightly) improved by using information about directivity.

\begin{table}[ht]\label{tab:dir}
\caption{Performance on \textit{normal-1,2,3,4} of ``Full Method'' with and
  without using information about directivity.}
\label{tab:directivity}

%\caption{Evaluation of different methods on the four \textit{normal} datasets\label{tab:comparison}}
\centering
\small
\begin{tabular}{| l | c c c c | c c c c |}
\hline
& \multicolumn{4}{c|}{AUROC} & \multicolumn{4}{c|}{AUPRC} \\
\textit{Full method} $\backslash$ \textit{normal-} & \textit{1} & \textit{2} & \textit{3} & \textit{4} & \textit{1} & \textit{2} & \textit{3} & \textit{4} \\
\hline
\hline
 Undirected & 0.943 & 0.942 & 0.942 & 0.939 & 0.403 & 0.404 & 0.398 & 0.388  \\
  Directed & \textbf{0.944} & \textbf{0.943} & 0.942 & \textbf{0.940} & \textbf{0.404} & \textbf{0.405} & \textbf{0.399} & \textbf{0.389}\\
\hline
\end{tabular}
\end{table}


\section{Supplementary results} \label{app:supp}

We report in this appendix the performance of the different methods compared
in the paper on 6 additional datasets provided by the challenge
organizers. These datasets, corresponding each to networks of 1000 neurons, are similar to
the \textit{normal} datasets except for one feature:
\begin{description}
\item[lowcon:] Similar network but with a lower number of connections per neuron on average.
\item[highcon:] Similar network but with a higher number of connections per neuron on average.
\item[lowcc:] Similar network but with a lower clustering coefficient on average.
\item[highcc:] Similar network but with a higher clustering coefficient on average.
\item[normal-3-highrate:] Same topology as \textit{normal-3} but with a higher firing frequency, i.e., with highly active neurons.
\item[normal-4-lownoise:] Same topology as \textit{normal-4} but with a better signal to noise ratio.
\end{description}

Results of several methods on these 6 datasets are provided in
Table~\ref{tab:results_appendix}. They confirm what we observed on the
\textit{normal} datasets. Average partial correlation and its tuned variant,
i.e.,``Full method'', clearly outperform other network inference methods on all
datasets. PC is close to GENIE3 and GTE but still slightly worse. GENIE3
performs better than GTE most of the times. Note that the "Full method" reported in this table does not use Equation \ref{eqn:qij} to slightly correct the values of $p_{i,j}$  to take into account directivity.

\begin{table}[h]
\caption{Performance (top: AUROC, bottom: AUPRC) on specific datasets with different methods.}
\label{tab:results_appendix}
\centering
\small
\begin{tabular}{| l | c c c c c c |}
\hline
& \multicolumn{6}{c|}{AUROC}\\
\textit{Method} $\backslash$ \textit{normal-} & \textit{lowcon} & \textit{highcon} & \textit{lowcc} & \textit{highcc} & \textit{3-highrate} & \textit{4-lownoise} \\
\hline
\hline
Averaging     & 0.947 & 0.943 & 0.920 & 0.942 & 0.959 & 0.934 \\
Full method   & \textbf{0.955} & \textbf{0.944} &  \textbf{0.925} & \textbf{0.946} & \textbf{0.961} & \textbf{0.941} \\
PC & 0.782 & 0.920 &  0.846 & 0.897  & 0.898  & 0.873 \\
GTE & 0.846 & 0.905 & 0.848 & 0.899 & 0.905 & 0.879\\
GENIE3 & 0.781 &  0.924 & 0.879 & 0.902 & 0.886 &  0.890 \\ \hline \hline
& \multicolumn{6}{c|}{AUPRC}\\ \hline
Averaging     & 0.320 & 0.429 & 0.262 & 0.478 & 0.443 & 0.412 \\
Full method   & \textbf{0.334} & \textbf{0.413} &  \textbf{0.260} & \textbf{0.486} & \textbf{0.452} & \textbf{0.432}\\
PC & 0.074 & 0.218 & 0.082 & 0.165  & 0.193 & 0.135 \\
GTE & 0.094 & 0.211 & 0.081 & 0.165 & 0.210 & 0.144\\
GENIE3 & 0.128 & 0.273 & 0.116 & 0.309 & 0.256 & 0.224\\ \hline
\end{tabular}
\end{table}

\section{About the number of principal components}
\label{app:pca}

\DIFaddbegin \DIFadd{As it can be seen on Figure~\ref{fig:pca}, only 800 components have a strictly positive value. Therefore, the value of the parameter $M$ is immediat and should be obviously set to $800$, or equivalently to $0.8p$. }\\

\DIFadd{Maybe an explanation about what does it mean to have a compoment equal to zero (to do). }\\

\DIFadd{Figure (to do).
}

\DIFaddend \section{Computational information}
\DIFaddbegin 


\begin{table}[h]
\caption{\DIFaddFL{Execution time, scalability, memory requirements for methods described in the paper. \textcolor{red}{to do.}}}
\label{tab:results_appendix}
\centering
\small
\begin{tabular}{| l|c|c|c|} \hline
\DIFaddFL{Methods }& \DIFaddFL{Execution time }& \DIFaddFL{Scalability }& \DIFaddFL{Memory requirements }\\ \hline
\DIFaddFL{Partial correlation }& & & \\ 
\DIFaddFL{GTE  }& & & \\
\DIFaddFL{GENIE3 }& & & \\ \hline
\end{tabular}
\end{table}
\DIFaddend 

\end{document}

